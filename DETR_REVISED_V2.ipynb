{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "asSMEwZnSxy_"
      },
      "source": [
        "------------------------------------------------------------------------\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "# Cell 1: Import Libraries and Setup\n",
        "\n",
        "This cell sets up our deep learning environment by importing necessary libraries and checking GPU availability.\n",
        "\n",
        "## Key Components:\n",
        "1. **Core Libraries**:\n",
        "   - `torch`: PyTorch deep learning framework\n",
        "   - `transformers`: Hugging Face library for pre-trained models\n",
        "   - `PIL`: Python Imaging Library for image processing\n",
        "   - `numpy`: Numerical computing library\n",
        "   - `pandas`: Data manipulation library\n",
        "\n",
        "2. **CUDA Check**:\n",
        "   - Verifies if GPU acceleration is available\n",
        "   - Prints GPU device information if available\n",
        "\n",
        "3. **Constants**:\n",
        "   - `NUM_CATEGORIES = 30`: Number of clothing categories\n",
        "   - `NUM_ATTRIBUTES = 341`: Number of possible attributes\n",
        "\n",
        "## Why These Numbers?\n",
        "- 30 categories cover main clothing types (shirts, pants, dresses, etc.)\n",
        "- 341 attributes include various features like colors, patterns, materials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2akqcd5wZUPo",
        "outputId": "984f3580-358d-4f4b-daf8-e882626f7036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.4.0\n",
            "CUDA available: True\n",
            "CUDA device: NVIDIA GeForce RTX 4090\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Import Libraries and Test CUDA\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DetrImageProcessor, DetrForSegmentation\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Test CUDA availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Constants\n",
        "NUM_CATEGORIES = 30\n",
        "NUM_ATTRIBUTES = 341"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "VgYt24aDSxzC"
      },
      "source": [
        "------------------------------------------------------------------------\n",
        "------------------------------------------------------------------------\n",
        "# Cell 2: Retrieve Data and Define Paths\n",
        "\n",
        "This cell sets up file paths for our dataset components.\n",
        "\n",
        "## Path Definitions:\n",
        "1. `LABEL_FILE`: JSON file containing label descriptions\n",
        "2. `CSV_FILE`: Training data annotations\n",
        "3. `IMAGE_DIR`: Directory containing training images\n",
        "\n",
        "## Important Note:\n",
        "There are two methods for retrieval...\n",
        "\n",
        "USE THE CORRECT METHOD:\n",
        "* (1) Kaggle Retrieval\n",
        "* (2) GPU Cluster Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAxWxyXhSxzC"
      },
      "source": [
        "### Use this for kaggle retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIUWsmJRnFFG"
      },
      "outputs": [],
      "source": [
        "# !mkdir ~/.kaggle #create the .kaggle folder in your root directory\n",
        "# !echo '{\"username\":\"YOUR_USERNAME\",\"key\":\"YOUR_KEY}' > ~/.kaggle/kaggle.json #write kaggle API credentials to kaggle.json\n",
        "  # !chmod 600 ~/.kaggle/kaggle.json  # set permissions\n",
        "# !pip install kaggle #install the kaggle library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccyEwJMlpRZY"
      },
      "outputs": [],
      "source": [
        "# !kaggle datasets list -s Fashionpediea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trklUfclqA1V"
      },
      "outputs": [],
      "source": [
        "# !kaggle datasets download -d chiangkhenghe/fashionpediea-fyp-full-train -p /content/kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkS-fO8HzgvG"
      },
      "outputs": [],
      "source": [
        "# ! unzip /content/kaggle/fashionpediea-fyp-full-train.zip -d /content/kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_uyeZGPZUPp"
      },
      "outputs": [],
      "source": [
        "# Define Paths\n",
        "LABEL_FILE = '/content/kaggle/label_description_v3.json'\n",
        "CSV_FILE = '/content/kaggle/train_swapaholic_v0.csv/train_swapaholic_v0.csv'\n",
        "IMAGE_DIR = '/content/kaggle/train'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "2aokMJ4NSxzE"
      },
      "source": [
        "### Use this when using GPU Cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jrXs8lGSxzE"
      },
      "outputs": [],
      "source": [
        "# # Define Paths\n",
        "# LABEL_FILE = r\"./imaterialist-2020/label_description_v3.json\"\n",
        "# CSV_FILE = r\"./imaterialist-2020/train_swapaholic_v0.csv\"\n",
        "# IMAGE_DIR = r\"./imaterialist-2020/train\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "qZnXJszoSxzE"
      },
      "source": [
        "------------------------------------------------------------------------\n",
        "------------------------------------------------------------------------\n",
        "# Cell 3: Category and Attribute Mappings\n",
        "\n",
        "This cell defines the classification structure for our fashion items.\n",
        "\n",
        "## Components:\n",
        "\n",
        "1. **CATEGORY_MAPPING**:\n",
        "   - Dictionary mapping numeric IDs to clothing categories\n",
        "   - Example: `0: \"shirt, blouse\"`, `1: \"top, t-shirt, sweatshirt\"`\n",
        "   - Total of 30 distinct categories\n",
        "\n",
        "2. **SUPERCATEGORIES**:\n",
        "   - Groups related categories together\n",
        "   - Helps in hierarchical classification\n",
        "   - Example: \"Tops\" includes shirts, t-shirts, and vests\n",
        "\n",
        "## Usage:\n",
        "- Used for converting between numeric IDs and human-readable labels\n",
        "- Helps in organizing and analyzing model predictions\n",
        "- Useful for generating reports and evaluating model performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbDWORNaZUPp"
      },
      "outputs": [],
      "source": [
        "# Category and attribute mappings\n",
        "CATEGORY_MAPPING = {\n",
        "    0: \"shirt, blouse\",\n",
        "    1: \"top, t-shirt, sweatshirt\",\n",
        "    2: \"sweater\",\n",
        "    3: \"cardigan\",\n",
        "    4: \"jacket\",\n",
        "    5: \"vest\",\n",
        "    6: \"pants\",\n",
        "    7: \"shorts\",\n",
        "    8: \"skirt\",\n",
        "    9: \"coat\",\n",
        "    10: \"dress\",\n",
        "    11: \"jumpsuit\",\n",
        "    12: \"cape\",\n",
        "    13: \"glasses\",\n",
        "    14: \"hat\",\n",
        "    15: \"headband, head covering, hair accessory\",\n",
        "    16: \"tie\",\n",
        "    17: \"glove\",\n",
        "    18: \"watch\",\n",
        "    19: \"belt\",\n",
        "    20: \"leg warmer\",\n",
        "    21: \"tights, stockings\",\n",
        "    22: \"sock\",\n",
        "    23: \"shoe\",\n",
        "    24: \"bag, wallet\",\n",
        "    25: \"scarf\",\n",
        "    26: \"umbrella\",\n",
        "    27: \"hood\",\n",
        "    28: \"epaulette\",\n",
        "    29: \"bow\"\n",
        "}\n",
        "\n",
        "# Group categories by supercategory\n",
        "SUPERCATEGORIES = {\n",
        "    \"Tops\": [0, 1, 5],\n",
        "    \"Outerwear\": [2, 3, 9, 12],\n",
        "    \"Blazers & Jackets\": [4],\n",
        "    \"Pants, Trousers, Leggings\": [6],\n",
        "    \"Shorts\": [7],\n",
        "    \"Skirts\": [8],\n",
        "    \"Dress\": [10],\n",
        "    \"Jumpsuits & Playsuits\": [11],\n",
        "    \"Accessories\": [28, 29]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "2Q3FgS3eSxzF"
      },
      "source": [
        "------------------------------------------------------------------------\n",
        "------------------------------------------------------------------------\n",
        "# Cell 4: Data Loading and Verification\n",
        "\n",
        "This cell loads and validates our dataset.\n",
        "\n",
        "## Operations:\n",
        "1. **Load CSV Data**:\n",
        "   - Reads training data into pandas DataFrame\n",
        "   - Displays basic dataset information\n",
        "\n",
        "2. **Data Validation**:\n",
        "   - Checks dataset shape (rows and columns)\n",
        "   - Lists available columns\n",
        "   - Shows sample data\n",
        "   - Identifies missing values\n",
        "\n",
        "## Why Important?\n",
        "- Ensures data quality before training\n",
        "- Helps identify potential issues early\n",
        "- Provides dataset statistics for reference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFF3_7nzZUPp",
        "outputId": "97032e0b-970f-4942-9877-7b7a25d0ef4f",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (90731, 6)\n",
            "\n",
            "Columns: ['ImageId', 'EncodedPixels', 'Height', 'Width', 'CategoryId', 'AttributesIds']\n",
            "\n",
            "Sample data:\n",
            "                            ImageId  \\\n",
            "0  00000663ed1ff0c4e0132b9b9ac53f6e   \n",
            "1  00000663ed1ff0c4e0132b9b9ac53f6e   \n",
            "2  00000663ed1ff0c4e0132b9b9ac53f6e   \n",
            "3  00000663ed1ff0c4e0132b9b9ac53f6e   \n",
            "4  00000663ed1ff0c4e0132b9b9ac53f6e   \n",
            "\n",
            "                                       EncodedPixels  Height  Width  \\\n",
            "0  6068157 7 6073371 20 6078584 34 6083797 48 608...    5214   3676   \n",
            "1  6323163 11 6328356 32 6333549 53 6338742 75 63...    5214   3676   \n",
            "2  8521389 10 8526585 30 8531789 42 8537002 46 85...    5214   3676   \n",
            "3  6421446 292 6426657 298 6431867 305 6437078 31...    5214   3676   \n",
            "4  4566382 8 4571592 25 4576803 41 4582013 58 458...    5214   3676   \n",
            "\n",
            "   CategoryId                       AttributesIds  \n",
            "0           6     115,136,143,154,230,295,316,317  \n",
            "1           0     115,136,142,146,225,295,316,317  \n",
            "2          28                                 163  \n",
            "3          29                                 174  \n",
            "4           4  17,115,136,145,149,225,295,311,317  \n",
            "\n",
            "Missing values:\n",
            "ImageId          0\n",
            "EncodedPixels    0\n",
            "Height           0\n",
            "Width            0\n",
            "CategoryId       0\n",
            "AttributesIds    0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Load and Check Data\n",
        "# Load your CSV file\n",
        "df = pd.read_csv(CSV_FILE)  # Replace with your path\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "print(\"\\nSample data:\")\n",
        "print(df.head())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing values:\")\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D-kk4qHZUPp"
      },
      "outputs": [],
      "source": [
        "# # Cell 3: Test Image Loading and RLE Decoding\n",
        "# def test_rle_decode(mask_rle, shape):\n",
        "#     \"\"\"Test RLE decoding for a single mask\"\"\"\n",
        "#     if pd.isna(mask_rle):\n",
        "#         return np.zeros(shape, dtype=np.bool_)\n",
        "\n",
        "#     s = mask_rle.split()\n",
        "#     starts = np.array(s[0::2], dtype=np.int64) - 1\n",
        "#     lengths = np.array(s[1::2], dtype=np.int64)\n",
        "#     mask = np.zeros(shape[0] * shape[1], dtype=np.bool_)\n",
        "\n",
        "#     for start, length in zip(starts, lengths):\n",
        "#         if start + length <= len(mask):\n",
        "#             mask[start:start + length] = True\n",
        "\n",
        "#     return mask.reshape(shape, order='F')\n",
        "\n",
        "# # Test with first image\n",
        "# first_image_id = df['ImageId'].iloc[0]  # Adjust based on your column name\n",
        "# first_mask_rle = df['EncodedPixels'].iloc[0]\n",
        "# first_category = df['CategoryId'].iloc[0]\n",
        "# first_attributes = df['AttributesIds'].iloc[0]\n",
        "\n",
        "# # Load and display image\n",
        "# first_image_path = f\"{IMAGE_DIR}/{first_image_id}.jpg\"\n",
        "# image = Image.open(first_image_path).convert('RGB')\n",
        "# width, height = image.size\n",
        "# print(f\"Image dimensions: {width}x{height}\")\n",
        "\n",
        "# # Test mask decoding\n",
        "# mask = test_rle_decode(first_mask_rle, (height, width))\n",
        "# print(f\"Mask shape: {mask.shape}\")\n",
        "# print(f\"Mask coverage: {mask.sum() / (height * width) * 100:.2f}%\")\n",
        "\n",
        "# # Visualize\n",
        "# plt.figure(figsize=(15, 5))\n",
        "# plt.subplot(131)\n",
        "# plt.imshow(image)\n",
        "# plt.title(\"Original Image\")\n",
        "# plt.axis('off')\n",
        "\n",
        "# plt.subplot(132)\n",
        "# plt.imshow(mask, cmap='gray')\n",
        "# plt.title(\"Decoded Mask\")\n",
        "# plt.axis('off')\n",
        "\n",
        "# plt.subplot(133)\n",
        "# plt.imshow(image)\n",
        "# plt.imshow(mask, alpha=0.5, cmap='Reds')\n",
        "# plt.title(\"Overlay\")\n",
        "# plt.axis('off')\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "FgIIn2u6SxzF"
      },
      "source": [
        "------------------------------------------------------------------------\n",
        "------------------------------------------------------------------------\n",
        "# Cell 5: Dataset Class Implementation\n",
        "\n",
        "This cell defines our custom dataset class for handling fashion images and their labels.\n",
        "\n",
        "## Class Overview: `FashionMultiTaskDataset`\n",
        "\n",
        "## Key Features:\n",
        "\n",
        "1. **Data Augmentation**:\n",
        "   - Random rotations and flips\n",
        "   - Brightness/contrast adjustments\n",
        "   - Random cropping\n",
        "   - Normalization\n",
        "\n",
        "2. **Label Processing**:\n",
        "   - Converts categories to one-hot encoding\n",
        "   - Processes attributes as multi-hot encoding\n",
        "   - Decodes RLE-encoded segmentation masks\n",
        "\n",
        "3. **Image Processing**:\n",
        "   - Resizes images to 800x800 pixels\n",
        "   - Applies transformations consistently\n",
        "   - Handles both training and validation modes\n",
        "\n",
        "## Methods:\n",
        "- `__init__`: Initializes dataset with paths and options\n",
        "- `rle_decode`: Converts RLE masks to binary format\n",
        "- `__getitem__`: Retrieves and processes single items\n",
        "- `__len__`: Returns dataset size\n",
        "\n",
        "## Usage:\n",
        "Used for both training and validation data loading with different augmentation settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JTMjTH1ZUPp",
        "outputId": "407dec06-864f-48ce-f56c-e4805711d022"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset item keys: dict_keys(['pixel_values', 'pixel_mask', 'category_labels', 'attribute_labels', 'mask_labels'])\n",
            "\n",
            "Shapes:\n",
            "pixel_values: torch.Size([3, 800, 800])\n",
            "pixel_mask: torch.Size([800, 800])\n",
            "category_labels: torch.Size([30])\n",
            "attribute_labels: torch.Size([341])\n",
            "mask_labels: torch.Size([800, 800])\n"
          ]
        }
      ],
      "source": [
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from transformers import DetrImageProcessor\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "\n",
        "class FashionMultiTaskDataset(Dataset):\n",
        "    def __init__(self, image_paths, masks, categories, attributes, image_processor, augment=False):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            image_paths (list): List of paths to images\n",
        "            masks (list): List of RLE encoded masks\n",
        "            categories (list): List of category IDs\n",
        "            attributes (list): List of attribute IDs (comma-separated strings)\n",
        "            image_processor: DETR image processor\n",
        "            augment (bool): Whether to apply data augmentation\n",
        "        \"\"\"\n",
        "        self.image_paths = image_paths\n",
        "        self.masks = masks\n",
        "        self.categories = categories\n",
        "        self.attributes = attributes\n",
        "        self.image_processor = image_processor\n",
        "        self.augment = augment\n",
        "        self.target_size = (800, 800)  # DETR default size\n",
        "\n",
        "        if augment:\n",
        "            self.transform = A.Compose([\n",
        "                A.RandomRotate90(p=0.5),\n",
        "                A.HorizontalFlip(p=0.5),\n",
        "                A.RandomBrightnessContrast(p=0.2),\n",
        "                A.RandomResizedCrop(\n",
        "                    height=self.target_size[0],\n",
        "                    width=self.target_size[1],\n",
        "                    scale=(0.8, 1.0)\n",
        "                ),\n",
        "                A.Normalize(\n",
        "                    mean=image_processor.image_mean,\n",
        "                    std=image_processor.image_std\n",
        "                ),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = A.Compose([\n",
        "                A.Resize(\n",
        "                    height=self.target_size[0],\n",
        "                    width=self.target_size[1]\n",
        "                ),\n",
        "                A.Normalize(\n",
        "                    mean=image_processor.image_mean,\n",
        "                    std=image_processor.image_std\n",
        "                ),\n",
        "                ToTensorV2()\n",
        "            ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def rle_decode(self, mask_rle, shape):\n",
        "        \"\"\"\n",
        "        Decode RLE encoded mask.\n",
        "\n",
        "        Args:\n",
        "            mask_rle (str): Run-length encoded mask\n",
        "            shape (tuple): Image shape (height, width)\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Binary mask\n",
        "        \"\"\"\n",
        "        if pd.isna(mask_rle):\n",
        "            return np.zeros(shape, dtype=np.bool_)\n",
        "\n",
        "        s = mask_rle.split()\n",
        "        starts = np.array(s[0::2], dtype=np.int64) - 1\n",
        "        lengths = np.array(s[1::2], dtype=np.int64)\n",
        "        mask = np.zeros(shape[0] * shape[1], dtype=np.bool_)\n",
        "\n",
        "        for start, length in zip(starts, lengths):\n",
        "            if start + length <= len(mask):\n",
        "                mask[start:start + length] = True\n",
        "\n",
        "        return mask.reshape(shape, order='F')\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get a single item from the dataset.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary containing:\n",
        "                - pixel_values: Image tensor\n",
        "                - pixel_mask: Attention mask\n",
        "                - category_labels: Category label\n",
        "                - attribute_labels: Attribute labels\n",
        "                - mask_labels: Segmentation mask\n",
        "        \"\"\"\n",
        "        # Load image\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        image = image.resize(self.target_size, Image.BILINEAR)\n",
        "\n",
        "        # Get original dimensions for mask decoding\n",
        "        original_width, original_height = image.size\n",
        "\n",
        "        # Decode mask\n",
        "        mask = self.rle_decode(self.masks[idx], (original_height, original_width))\n",
        "        mask = torch.from_numpy(mask).float()\n",
        "\n",
        "        # Resize mask to target size\n",
        "        mask = mask.unsqueeze(0)  # Add channel dimension\n",
        "        mask = nn.functional.interpolate(\n",
        "            mask.unsqueeze(0),  # Add batch dimension\n",
        "            size=self.target_size,\n",
        "            mode='nearest'\n",
        "        ).squeeze(0).squeeze(0)  # Remove batch and channel dimensions\n",
        "\n",
        "        # Process image with image processor\n",
        "        encoding = self.image_processor(\n",
        "            images=image,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Remove batch dimension\n",
        "        for k,v in encoding.items():\n",
        "            encoding[k] = v.squeeze()\n",
        "\n",
        "        # Convert category to one-hot\n",
        "        category = torch.zeros(NUM_CATEGORIES)\n",
        "        category[self.categories[idx]] = 1\n",
        "\n",
        "        # Convert attributes to multi-hot\n",
        "        attributes = torch.zeros(NUM_ATTRIBUTES)\n",
        "        if isinstance(self.attributes[idx], str):\n",
        "            attr_indices = [int(x) for x in self.attributes[idx].split(',')]\n",
        "        else:\n",
        "            attr_indices = self.attributes[idx]\n",
        "        attributes[attr_indices] = 1\n",
        "\n",
        "        # Add labels to encoding\n",
        "        encoding['category_labels'] = category\n",
        "        encoding['attribute_labels'] = attributes\n",
        "        encoding['mask_labels'] = mask\n",
        "\n",
        "        return encoding\n",
        "\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Custom collate function for DataLoader.\n",
        "\n",
        "    Args:\n",
        "        batch (list): List of samples from dataset\n",
        "\n",
        "    Returns:\n",
        "        dict: Batched samples\n",
        "    \"\"\"\n",
        "    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n",
        "    pixel_mask = torch.stack([item['pixel_mask'] for item in batch])\n",
        "    category_labels = torch.stack([item['category_labels'] for item in batch])\n",
        "    attribute_labels = torch.stack([item['attribute_labels'] for item in batch])\n",
        "    mask_labels = torch.stack([item['mask_labels'] for item in batch])\n",
        "\n",
        "    return {\n",
        "        'pixel_values': pixel_values,\n",
        "        'pixel_mask': pixel_mask,\n",
        "        'category_labels': category_labels,\n",
        "        'attribute_labels': attribute_labels,\n",
        "        'mask_labels': mask_labels\n",
        "    }\n",
        "\n",
        "# Test Dataset\n",
        "image_processor = DetrImageProcessor.from_pretrained('facebook/detr-resnet-50-panoptic')\n",
        "\n",
        "# Create small test dataset\n",
        "image_paths = [f\"{IMAGE_DIR}/{image_id}.jpg\" for image_id in df['ImageId'].iloc[:5]]\n",
        "test_dataset = FashionMultiTaskDataset(\n",
        "    image_paths=image_paths,\n",
        "    masks=df['EncodedPixels'].iloc[:5],\n",
        "    categories=df['CategoryId'].iloc[:5],\n",
        "    attributes=df['AttributesIds'].iloc[:5],\n",
        "    image_processor=image_processor\n",
        ")\n",
        "\n",
        "# Test loading one item\n",
        "test_item = test_dataset[0]\n",
        "print(\"Dataset item keys:\", test_item.keys())\n",
        "print(\"\\nShapes:\")\n",
        "for k, v in test_item.items():\n",
        "    if isinstance(v, torch.Tensor):\n",
        "        print(f\"{k}: {v.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "Bi2Yr8ymSxzG"
      },
      "source": [
        "------------------------------------------------------------------------\n",
        "------------------------------------------------------------------------\n",
        "# Cell 6: Model Architecture\n",
        "\n",
        "This cell defines our core model architecture based on DETR (Detection Transformer).\n",
        "\n",
        "## Class Overview: `FashionMultiTaskDETR`\n",
        "\n",
        "## Architecture Components:\n",
        "\n",
        "1. **Base Model**:\n",
        "   - Uses pretrained DETR (facebook/detr-resnet-50-panoptic)\n",
        "   - Leverages transformer architecture for image understanding\n",
        "   - Hidden dimension: 256 (from DETR config)\n",
        "\n",
        "2. **Category Head**:\n",
        "   ```python\n",
        "   self.category_head = nn.Sequential(\n",
        "       nn.Linear(hidden_dim, hidden_dim),\n",
        "       nn.BatchNorm1d(hidden_dim),\n",
        "       nn.ReLU(),\n",
        "       nn.Dropout(0.3),\n",
        "       # ... additional layers\n",
        "   )\n",
        "   ```\n",
        "   - Deep network for category classification\n",
        "   - Uses batch normalization for stable training\n",
        "   - Dropout (0.3) for regularization\n",
        "   - Progressive dimension reduction\n",
        "\n",
        "3. **Attribute Head**:\n",
        "   - Simpler architecture for multi-label classification\n",
        "   - Lower dropout (0.1) to maintain feature richness\n",
        "   - Direct mapping to attribute space\n",
        "\n",
        "4. **Forward Pass Logic**:\n",
        "   - Processes images through DETR backbone\n",
        "   - Extracts global features for classification\n",
        "   - Generates predictions for all three tasks\n",
        "\n",
        "## Why This Architecture?\n",
        "- Balances complexity across tasks\n",
        "- Prevents overfitting through regularization\n",
        "- Maintains pretrained knowledge while adding fashion-specific capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZuocWbmZUPp"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Model Class and Test\n",
        "\n",
        "class FashionMultiTaskDETR(nn.Module):\n",
        "    def __init__(self, num_categories=NUM_CATEGORIES, num_attributes=NUM_ATTRIBUTES):\n",
        "        super().__init__()\n",
        "\n",
        "        self.detr = DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\n",
        "        hidden_dim = self.detr.config.d_model\n",
        "\n",
        "        # Modified category head with batch norm and dropout\n",
        "        self.category_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.BatchNorm1d(hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(hidden_dim // 2, num_categories)\n",
        "        )\n",
        "\n",
        "        self.attribute_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim, num_attributes)\n",
        "        )\n",
        "\n",
        "    def forward(self, pixel_values, pixel_mask):\n",
        "        # Get DETR outputs\n",
        "        outputs = self.detr(\n",
        "            pixel_values=pixel_values,\n",
        "            pixel_mask=pixel_mask\n",
        "        )\n",
        "\n",
        "        # Get last hidden state for classification tasks\n",
        "        last_hidden_state = outputs.last_hidden_state  # Shape: [batch_size, 100, 256]\n",
        "        global_feature = last_hidden_state[:, 0]  # Use first token as global feature\n",
        "\n",
        "        # Task-specific predictions\n",
        "        category_logits = self.category_head(global_feature)\n",
        "        attribute_logits = self.attribute_head(global_feature)\n",
        "\n",
        "        # Get segmentation predictions\n",
        "        # pred_masks shape: [batch_size, num_queries, height, width]\n",
        "        masks = outputs.pred_masks\n",
        "\n",
        "        # Aggregate masks if needed (you might want to adjust this based on your needs)\n",
        "        # Here we're taking the max across all queries\n",
        "        segmentation_mask = torch.max(masks, dim=1)[0]  # Shape: [batch_size, height, width]\n",
        "\n",
        "        return {\n",
        "            'category_logits': category_logits,\n",
        "            'attribute_logits': attribute_logits,\n",
        "            'segmentation_masks': segmentation_mask\n",
        "        }\n",
        "\n",
        "# Test Model\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize model and move to device\n",
        "model = FashionMultiTaskDETR().to(device)\n",
        "\n",
        "# Create sample batch\n",
        "sample_batch = {\n",
        "    'pixel_values': torch.randn(2, 3, 800, 800).to(device),\n",
        "    'pixel_mask': torch.ones(2, 800, 800).to(device)\n",
        "}\n",
        "\n",
        "# Test forward pass\n",
        "with torch.no_grad():\n",
        "    outputs = model(**sample_batch)\n",
        "\n",
        "print(\"\\nModel output shapes:\")\n",
        "for k, v in outputs.items():\n",
        "    if isinstance(v, torch.Tensor):\n",
        "        print(f\"{k}: {v.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "6L4XE-_qSxzG"
      },
      "source": [
        "------------------------------------------------------------------------\n",
        "------------------------------------------------------------------------\n",
        "# Cell 7: Loss Function Implementation\n",
        "\n",
        "This cell defines our multi-task loss function that balances learning across different objectives.\n",
        "\n",
        "## Class Overview: `MultitaskLoss`\n",
        "\n",
        "## Loss Components:\n",
        "\n",
        "1. **Learnable Task Weights**:\n",
        "   ```python\n",
        "   self.log_vars = nn.Parameter(torch.FloatTensor([-0.5, 0.0, 0.5]))\n",
        "   ```\n",
        "   - Automatically learns importance of each task\n",
        "   - Converts to weights through exponential function\n",
        "   - Initialized with different values for each task\n",
        "\n",
        "2. **Category Loss**:\n",
        "   - Binary Cross Entropy with Logits\n",
        "   - Enhanced positive weighting (5.0)\n",
        "   - Handles class imbalance\n",
        "\n",
        "3. **Attribute Loss**:\n",
        "   - Similar to category loss but with lower positive weight (2.0)\n",
        "   - Designed for multi-label scenario\n",
        "   - Balanced for attribute frequency\n",
        "\n",
        "4. **Segmentation Loss**:\n",
        "   - Standard BCE loss for pixel-wise predictions\n",
        "   - Equal weighting for all pixels\n",
        "   - Includes regularization term\n",
        "\n",
        "## Loss Calculation:\n",
        "- Combines all three losses with learned weights\n",
        "- Includes temperature scaling for better calibration\n",
        "- Adds L2 regularization for stability\n",
        "\n",
        "## Why This Approach?\n",
        "- Automatic task balancing\n",
        "- Handles different scales of losses\n",
        "- Adapts to changing task difficulties during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HivBWY55ZUPp"
      },
      "outputs": [],
      "source": [
        "# Cell 6: Loss Function\n",
        "class MultitaskLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Initialize log variances with different values\n",
        "        self.log_vars = nn.Parameter(torch.FloatTensor([-0.5, 0.0, 0.5]))\n",
        "\n",
        "        # Modify BCE loss for categories with stronger positive weighting\n",
        "        self.category_loss_fn = nn.BCEWithLogitsLoss(\n",
        "            pos_weight=torch.ones(NUM_CATEGORIES) * 5.0,  # Increase positive weight\n",
        "            reduction='none'\n",
        "        )\n",
        "\n",
        "        # Rest remains the same\n",
        "        self.attribute_loss_fn = nn.BCEWithLogitsLoss(\n",
        "            pos_weight=torch.ones(NUM_ATTRIBUTES) * 2.0,\n",
        "            reduction='none'\n",
        "        )\n",
        "        self.segmentation_loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
        "\n",
        "    def to(self, device):\n",
        "        super().to(device)\n",
        "        self.attribute_loss_fn.pos_weight = self.attribute_loss_fn.pos_weight.to(device)\n",
        "        return self\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        device = outputs['category_logits'].device\n",
        "\n",
        "        # Add L2 regularization for category predictions\n",
        "        l2_reg = 0.01 * torch.norm(outputs['category_logits'])\n",
        "\n",
        "        # Calculate losses with temperature scaling\n",
        "        temperature = 2.0\n",
        "        category_loss = self.category_loss_fn(\n",
        "            outputs['category_logits'] / temperature,\n",
        "            targets['category_labels'].to(device).float() * 0.9\n",
        "        ).mean() + l2_reg\n",
        "\n",
        "        # Calculate raw losses first\n",
        "        category_loss = self.category_loss_fn(\n",
        "            outputs['category_logits'],\n",
        "            targets['category_labels'].to(device).float() * 0.9\n",
        "        ).mean()\n",
        "\n",
        "        attribute_loss = self.attribute_loss_fn(\n",
        "            outputs['attribute_logits'],\n",
        "            targets['attribute_labels'].to(device)\n",
        "        ).mean()\n",
        "\n",
        "        pred_masks = outputs['segmentation_masks']\n",
        "        target_masks = targets['mask_labels'].to(device).float()\n",
        "        target_masks = nn.functional.interpolate(\n",
        "            target_masks.unsqueeze(1),\n",
        "            size=pred_masks.shape[-2:],\n",
        "            mode='nearest'\n",
        "        ).squeeze(1)\n",
        "\n",
        "        segmentation_loss = self.segmentation_loss_fn(\n",
        "            pred_masks,\n",
        "            target_masks\n",
        "        ).mean()\n",
        "\n",
        "        # Calculate precision terms\n",
        "        precision_category = torch.exp(-self.log_vars[0])\n",
        "        precision_attribute = torch.exp(-self.log_vars[1])\n",
        "        precision_segmentation = torch.exp(-self.log_vars[2])\n",
        "\n",
        "        # Calculate weighted losses\n",
        "        weighted_category_loss = precision_category * category_loss + 0.5 * self.log_vars[0]\n",
        "        weighted_attribute_loss = precision_attribute * attribute_loss + 0.5 * self.log_vars[1]\n",
        "        weighted_segmentation_loss = precision_segmentation * segmentation_loss + 0.5 * self.log_vars[2]\n",
        "\n",
        "        # Store all components\n",
        "        losses = {\n",
        "            'total_loss': weighted_category_loss + weighted_attribute_loss + weighted_segmentation_loss,\n",
        "            'raw_category_loss': category_loss.item(),\n",
        "            'raw_attribute_loss': attribute_loss.item(),\n",
        "            'raw_segmentation_loss': segmentation_loss.item(),\n",
        "            'category_weight': precision_category.item(),\n",
        "            'attribute_weight': precision_attribute.item(),\n",
        "            'segmentation_weight': precision_segmentation.item()\n",
        "        }\n",
        "\n",
        "        return losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "xngwcFYaSxzH"
      },
      "source": [
        "------------------------------------------------------------------------\n",
        "------------------------------------------------------------------------\n",
        "# Cell 8: MultitaskTrainer Implementation\n",
        "\n",
        "This cell implements a comprehensive training framework for our multi-task fashion model.\n",
        "\n",
        "## Class Overview: `MultitaskTrainer`\n",
        "\n",
        "### Initialization Parameters\n",
        "```python\n",
        "def __init__(\n",
        "    self,\n",
        "    model,\n",
        "    train_dataset,\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    learning_rate=2e-4,\n",
        "    num_epochs=20,\n",
        "    device=None,\n",
        "    patience=5,\n",
        "    max_grad_norm=1.0,\n",
        "    warmup_ratio=0.1,\n",
        "    checkpoint_dir=None,\n",
        "    logger=None,\n",
        "    gradient_accumulation_steps=4\n",
        "):\n",
        "```\n",
        "\n",
        "### Key Components:\n",
        "\n",
        "1. **Memory Management**:\n",
        "   - GPU cache clearing\n",
        "   - Memory-efficient DataLoaders\n",
        "   - Gradient accumulation for large batches\n",
        "   ```python\n",
        "   if torch.cuda.is_available():\n",
        "       torch.cuda.empty_cache()\n",
        "   ```\n",
        "\n",
        "2. **Optimizer Configuration**:\n",
        "   - Three parameter groups with different learning rates:\n",
        "     - DETR backbone: 0.05× base learning rate\n",
        "     - Category head: 2.0× base learning rate\n",
        "     - Other components: Base learning rate\n",
        "   ```python\n",
        "   param_dicts = [\n",
        "       {\"params\": [...], \"lr\": learning_rate * 0.05},  # DETR backbone\n",
        "       {\"params\": [...], \"lr\": learning_rate * 2.0},   # Category head\n",
        "       {\"params\": [...], \"lr\": learning_rate}          # Others\n",
        "   ]\n",
        "   ```\n",
        "\n",
        "3. **Training Features**:\n",
        "   - Checkpoint saving/loading\n",
        "   - Early stopping\n",
        "   - Learning rate scheduling\n",
        "   - Gradient clipping\n",
        "   - Comprehensive metrics tracking\n",
        "\n",
        "### Key Methods:\n",
        "\n",
        "1. **train()**:\n",
        "   - Main training loop\n",
        "   - Epoch-level training\n",
        "   - Validation\n",
        "   - Metrics logging\n",
        "   - Checkpoint management\n",
        "\n",
        "2. **train_epoch()**:\n",
        "   - Single epoch training\n",
        "   - Gradient accumulation\n",
        "   - Memory optimization\n",
        "   - Progress tracking\n",
        "\n",
        "3. **evaluate()**:\n",
        "   - Validation loop\n",
        "   - Metrics computation\n",
        "   - No gradient calculation\n",
        "\n",
        "## Usage Example:\n",
        "```python\n",
        "trainer = MultitaskTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    val_dataset=val_dataset,\n",
        "    batch_size=4,\n",
        "    learning_rate=2e-4,\n",
        "    num_epochs=20\n",
        ")\n",
        "history = trainer.train()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukZO26grrXQo"
      },
      "outputs": [],
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "from collections import defaultdict\n",
        "\n",
        "# Cell 8: Updated Training Setup with Metrics\n",
        "class MultitaskTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model,\n",
        "        train_dataset,\n",
        "        val_dataset,\n",
        "        batch_size=4,\n",
        "        learning_rate=2e-4,\n",
        "        num_epochs=20,\n",
        "        device=None,\n",
        "        patience=5,\n",
        "        max_grad_norm=1.0,\n",
        "        warmup_ratio=0.1,\n",
        "        checkpoint_dir=None,\n",
        "        logger=None,\n",
        "        gradient_accumulation_steps=4\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the trainer with memory optimization.\n",
        "        \"\"\"\n",
        "        self.device = device if device else torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.model = model.to(self.device)\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "        self.patience = patience\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
        "        self.best_val_loss = float('inf')\n",
        "        self.patience_counter = 0\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.logger = logger or logging.getLogger(__name__)\n",
        "\n",
        "        # Clear GPU cache\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            self.logger.info(f\"GPU Memory After Cache Clear: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
        "\n",
        "        # DataLoaders with pin_memory=False to reduce memory usage\n",
        "        self.train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=2,\n",
        "            pin_memory=False,\n",
        "            collate_fn=custom_collate_fn\n",
        "        )\n",
        "\n",
        "        self.val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=batch_size,\n",
        "            num_workers=2,\n",
        "            pin_memory=False,\n",
        "            collate_fn=custom_collate_fn\n",
        "        )\n",
        "\n",
        "        # Loss and Optimizer\n",
        "        self.criterion = MultitaskLoss().to(self.device)\n",
        "\n",
        "        # Parameter groups with different learning rates\n",
        "        param_dicts = [\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters()\n",
        "                          if \"detr\" in n and p.requires_grad],\n",
        "                \"lr\": learning_rate * 0.05  # Reduce backbone learning rate\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters()\n",
        "                          if \"category_head\" in n and p.requires_grad],\n",
        "                \"lr\": learning_rate * 2.0  # Increase category head learning rate\n",
        "            },\n",
        "            {\n",
        "                \"params\": [p for n, p in model.named_parameters()\n",
        "                          if \"detr\" not in n and \"category_head\" not in n and p.requires_grad],\n",
        "                \"lr\": learning_rate\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        # Update optimizer with weight decay\n",
        "        self.optimizer = torch.optim.AdamW(\n",
        "            param_dicts,\n",
        "            weight_decay=0.05  # Increase weight decay\n",
        "        )\n",
        "\n",
        "        # Learning rate scheduler with warmup\n",
        "        num_training_steps = self.num_epochs * len(self.train_loader)\n",
        "        num_warmup_steps = int(num_training_steps * warmup_ratio)\n",
        "\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            self.optimizer,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            num_training_steps=num_training_steps\n",
        "        )\n",
        "\n",
        "        # Initialize metrics tracking\n",
        "        self.train_metrics_history = []\n",
        "        self.val_metrics_history = []\n",
        "\n",
        "        # Log training setup\n",
        "        self.logger.info(f\"\\nTraining Setup:\")\n",
        "        self.logger.info(f\"Batch Size: {batch_size}\")\n",
        "        self.logger.info(f\"Learning Rate: {learning_rate}\")\n",
        "        self.logger.info(f\"Number of Epochs: {num_epochs}\")\n",
        "        self.logger.info(f\"Training Steps per Epoch: {len(self.train_loader)}\")\n",
        "        self.logger.info(f\"Total Training Steps: {num_training_steps}\")\n",
        "        self.logger.info(f\"Warmup Steps: {num_warmup_steps}\")\n",
        "\n",
        "    def save_checkpoint(self, epoch, metrics, is_best=False):\n",
        "        \"\"\"\n",
        "        Save training checkpoint\n",
        "\n",
        "        Args:\n",
        "            epoch: Current epoch number\n",
        "            metrics: Dictionary of current metrics\n",
        "            is_best: Whether this checkpoint has the best validation loss\n",
        "        \"\"\"\n",
        "        if self.checkpoint_dir is None:\n",
        "            return\n",
        "\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'metrics': metrics,\n",
        "            'best_val_loss': self.best_val_loss\n",
        "        }\n",
        "\n",
        "        # Save latest checkpoint\n",
        "        checkpoint_path = f'{self.checkpoint_dir}/checkpoint_latest.pth'\n",
        "        torch.save(checkpoint, checkpoint_path)\n",
        "        self.logger.info(f\"Saved checkpoint: {checkpoint_path}\")\n",
        "\n",
        "        # Save best model separately\n",
        "        if is_best:\n",
        "            best_model_path = f'{self.checkpoint_dir}/model_best.pth'\n",
        "            torch.save(checkpoint, best_model_path)\n",
        "            self.logger.info(f\"Saved best model: {best_model_path}\")\n",
        "\n",
        "        # Save epoch-specific checkpoint periodically\n",
        "        if (epoch + 1) % 5 == 0:  # Save every 5 epochs\n",
        "            epoch_checkpoint_path = f'{self.checkpoint_dir}/checkpoint_epoch_{epoch+1}.pth'\n",
        "            torch.save(checkpoint, epoch_checkpoint_path)\n",
        "            self.logger.info(f\"Saved epoch checkpoint: {epoch_checkpoint_path}\")\n",
        "\n",
        "    def load_checkpoint(self, checkpoint_path):\n",
        "        \"\"\"\n",
        "        Load a training checkpoint\n",
        "\n",
        "        Args:\n",
        "            checkpoint_path: Path to the checkpoint file\n",
        "        \"\"\"\n",
        "        self.logger.info(f\"Loading checkpoint: {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n",
        "\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        self.best_val_loss = checkpoint['best_val_loss']\n",
        "\n",
        "        return checkpoint['epoch']\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Main training loop with enhanced logging\n",
        "        \"\"\"\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.logger.info(f\"\\nEpoch {epoch+1}/{self.num_epochs}\")\n",
        "\n",
        "            # Training phase\n",
        "            train_loss, train_metrics = self.train_epoch()\n",
        "            train_losses.append(train_loss)\n",
        "            self.train_metrics_history.append(train_metrics)\n",
        "\n",
        "            # Validation phase\n",
        "            val_loss, val_metrics = self.evaluate()\n",
        "            val_losses.append(val_loss)\n",
        "            self.val_metrics_history.append(val_metrics)\n",
        "\n",
        "            # Enhanced logging\n",
        "            self.logger.info(\"\\nTraining Metrics:\")\n",
        "            self.logger.info(f\"Total Loss: {train_loss:.4f}\")\n",
        "            self.logger.info(\"Task Losses:\")\n",
        "            for task in ['category', 'attribute', 'segmentation']:\n",
        "                self.logger.info(f\"  {task}: {train_metrics[f'{task}_loss']:.4f} \"\n",
        "                               f\"(weight: {train_metrics[f'{task}_weight']:.4f})\")\n",
        "            self.logger.info(\"Performance Metrics:\")\n",
        "            for k, v in train_metrics.items():\n",
        "                if not (k.endswith('_loss') or k.endswith('_weight')):\n",
        "                    self.logger.info(f\"  {k}: {v:.4f}\")\n",
        "\n",
        "            # Log metrics\n",
        "            self.logger.info(f\"Train Loss: {train_loss:.4f}\")\n",
        "            self.logger.info(\"Train Metrics: %s\",\n",
        "                           {k: f\"{v:.4f}\" for k, v in train_metrics.items()})\n",
        "            self.logger.info(f\"Val Loss: {val_loss:.4f}\")\n",
        "            self.logger.info(\"Val Metrics: %s\",\n",
        "                           {k: f\"{v:.4f}\" for k, v in val_metrics.items()})\n",
        "            self.logger.info(f\"Learning Rate: {self.scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "            # Save checkpoint\n",
        "            is_best = val_loss < self.best_val_loss\n",
        "            if is_best:\n",
        "                self.best_val_loss = val_loss\n",
        "                self.patience_counter = 0\n",
        "            else:\n",
        "                self.patience_counter += 1\n",
        "\n",
        "            self.save_checkpoint(\n",
        "                epoch=epoch,\n",
        "                metrics={'train': train_metrics, 'val': val_metrics},\n",
        "                is_best=is_best\n",
        "            )\n",
        "\n",
        "            # Early stopping\n",
        "            if self.patience_counter >= self.patience:\n",
        "                self.logger.info(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            'train_losses': train_losses,\n",
        "            'val_losses': val_losses,\n",
        "            'train_metrics_history': self.train_metrics_history,\n",
        "            'val_metrics_history': self.val_metrics_history\n",
        "        }\n",
        "\n",
        "    def train_epoch(self):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        total_metrics = defaultdict(float)\n",
        "        total_raw_losses = defaultdict(float)\n",
        "        total_weights = defaultdict(float)\n",
        "\n",
        "        progress_bar = tqdm(self.train_loader, desc='Training')\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "            outputs = self.model(\n",
        "                pixel_values=batch['pixel_values'],\n",
        "                pixel_mask=batch['pixel_mask']\n",
        "            )\n",
        "\n",
        "            losses = self.criterion(outputs, batch)\n",
        "            loss = losses['total_loss'] / self.gradient_accumulation_steps\n",
        "\n",
        "            # Track raw losses and weights\n",
        "            for task in ['category', 'attribute', 'segmentation']:\n",
        "                total_raw_losses[f'{task}_loss'] += losses[f'raw_{task}_loss']\n",
        "                total_weights[f'{task}_weight'] += losses[f'{task}_weight']\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            metrics = self.compute_metrics(outputs, batch)\n",
        "            total_loss += losses['total_loss'].item()\n",
        "\n",
        "            for k, v in metrics.items():\n",
        "                total_metrics[k] += v\n",
        "\n",
        "            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
        "                self.optimizer.step()\n",
        "                self.scheduler.step()\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "            if batch_idx % 500 == 0:\n",
        "                self.logger.info(f\"\\nStep {batch_idx} log_vars: {self.criterion.log_vars.data.tolist()}\")\n",
        "                self.logger.info(f\"Category pred mean: {metrics['cat_pred_mean']:.3f}, \"\n",
        "                                f\"target mean: {metrics['cat_target_mean']:.3f}\")\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'total': f\"{losses['total_loss']:.3f}\",\n",
        "                'raw_losses': f\"c:{losses['raw_category_loss']:.3f}/a:{losses['raw_attribute_loss']:.3f}/s:{losses['raw_segmentation_loss']:.3f}\",\n",
        "                'weights': f\"c:{losses['category_weight']:.3f}/a:{losses['attribute_weight']:.3f}/s:{losses['segmentation_weight']:.3f}\",\n",
        "                'metrics': f\"c:{metrics['category_dice']:.3f}({metrics['cat_pred_mean']:.2f})/a:{metrics['attribute_dice']:.3f}/s:{metrics['segmentation_miou']:.3f}\"\n",
        "            })\n",
        "\n",
        "            del outputs, losses, loss\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        num_batches = len(self.train_loader)\n",
        "        avg_metrics = {k: v / num_batches for k, v in total_metrics.items()}\n",
        "        avg_raw_losses = {k: v / num_batches for k, v in total_raw_losses.items()}\n",
        "        avg_weights = {k: v / num_batches for k, v in total_weights.items()}\n",
        "\n",
        "        # Combine all metrics\n",
        "        avg_metrics.update(avg_raw_losses)\n",
        "        avg_metrics.update(avg_weights)\n",
        "\n",
        "        return total_loss / num_batches, avg_metrics\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Validation loop with enhanced logging\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_metrics = defaultdict(float)\n",
        "        total_task_losses = defaultdict(float)\n",
        "        total_weights = defaultdict(float)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm(self.val_loader, desc='Validating')\n",
        "            for batch in progress_bar:\n",
        "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
        "                outputs = self.model(\n",
        "                    pixel_values=batch['pixel_values'],\n",
        "                    pixel_mask=batch['pixel_mask']\n",
        "                )\n",
        "\n",
        "                losses = self.criterion(outputs, batch)\n",
        "                total_loss += losses['total_loss'].item()\n",
        "\n",
        "                metrics = self.compute_metrics(outputs, batch)\n",
        "                for k, v in metrics.items():\n",
        "                    total_metrics[k] += v\n",
        "\n",
        "                # Track individual losses and weights\n",
        "                for k in ['category', 'attribute', 'segmentation']:\n",
        "                    total_task_losses[f'{k}_loss'] += losses[f'raw_{k}_loss']\n",
        "                    total_weights[f'{k}_weight'] += losses[f'{k}_weight']\n",
        "\n",
        "                progress_bar.set_postfix({\n",
        "                    'total': f\"{losses['total_loss']:.3f}\",\n",
        "                    'cat': f\"{losses['raw_category_loss']:.3f}({losses['category_weight']:.2f})\",\n",
        "                    'attr': f\"{losses['raw_attribute_loss']:.3f}({losses['attribute_weight']:.2f})\",\n",
        "                    'seg': f\"{losses['raw_segmentation_loss']:.3f}({losses['segmentation_weight']:.2f})\",\n",
        "                    'metrics': ', '.join([f\"{k[:3]}: {v:.3f}\" for k, v in metrics.items()])\n",
        "                })\n",
        "\n",
        "            num_batches = len(self.val_loader)\n",
        "            avg_metrics = {k: v / num_batches for k, v in total_metrics.items()}\n",
        "            avg_losses = {k: v / num_batches for k, v in total_task_losses.items()}\n",
        "            avg_weights = {k: v / num_batches for k, v in total_weights.items()}\n",
        "\n",
        "            # Add losses and weights to metrics for logging\n",
        "            avg_metrics.update(avg_losses)\n",
        "            avg_metrics.update(avg_weights)\n",
        "\n",
        "            return total_loss / num_batches, avg_metrics\n",
        "\n",
        "    def compute_metrics(self, outputs, targets):\n",
        "        device = outputs['category_logits'].device\n",
        "        metrics = {}\n",
        "\n",
        "        # Category Dice Score\n",
        "        cat_probs = torch.sigmoid(outputs['category_logits'])\n",
        "        cat_preds = (cat_probs > 0.5).float()\n",
        "        cat_targets = targets['category_labels'].to(device).float()\n",
        "\n",
        "        # Calculate Dice score per sample and then average\n",
        "        cat_intersection = (cat_preds * cat_targets).sum(dim=1)\n",
        "        cat_union = cat_preds.sum(dim=1) + cat_targets.sum(dim=1)\n",
        "        cat_dice = (2 * cat_intersection + 1e-8) / (cat_union + 1e-8)\n",
        "        metrics['category_dice'] = cat_dice.mean().item()\n",
        "\n",
        "        # Add prediction statistics for debugging\n",
        "        metrics['cat_pred_mean'] = cat_preds.mean().item()\n",
        "        metrics['cat_target_mean'] = cat_targets.mean().item()\n",
        "\n",
        "        # Attribute Dice Score (no changes needed)\n",
        "        attr_probs = torch.sigmoid(outputs['attribute_logits'])\n",
        "        attr_preds = (attr_probs > 0.5).float()\n",
        "        attr_targets = targets['attribute_labels'].to(device).float()\n",
        "\n",
        "        attr_intersection = (attr_preds * attr_targets).sum(dim=1)\n",
        "        attr_union = attr_preds.sum(dim=1) + attr_targets.sum(dim=1)\n",
        "        attr_dice = (2 * attr_intersection + 1e-8) / (attr_union + 1e-8)\n",
        "        metrics['attribute_dice'] = attr_dice.mean().item()\n",
        "\n",
        "        # Segmentation mIOU (no changes needed)\n",
        "        pred_masks = torch.sigmoid(outputs['segmentation_masks']) > 0.5\n",
        "        target_masks = targets['mask_labels'].to(device)\n",
        "\n",
        "        if pred_masks.shape != target_masks.shape:\n",
        "            target_masks = nn.functional.interpolate(\n",
        "                target_masks.unsqueeze(1).float(),\n",
        "                size=pred_masks.shape[-2:],\n",
        "                mode='nearest'\n",
        "            ).squeeze(1)\n",
        "\n",
        "        intersection = (pred_masks & target_masks.bool()).float().sum((1, 2))\n",
        "        union = (pred_masks | target_masks.bool()).float().sum((1, 2))\n",
        "        batch_ious = (intersection + 1e-8) / (union + 1e-8)\n",
        "        metrics['segmentation_miou'] = batch_ious.mean().item()\n",
        "\n",
        "        return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spgLYzTgSxzH"
      },
      "source": [
        "------------------------------------------------------------------------\n",
        "------------------------------------------------------------------------\n",
        "# Cell 9: Training Utilities and Pipeline\n",
        "\n",
        "This cell implements supporting functions for training visualization, logging, and pipeline execution.\n",
        "\n",
        "## Key Components:\n",
        "\n",
        "### 1. Training History Visualization\n",
        "```python\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Creates comprehensive training progress plots\"\"\"\n",
        "```\n",
        "Features:\n",
        "- Multiple subplots for each metric\n",
        "- Loss curves\n",
        "- Training vs validation comparison\n",
        "- Timestamp-labeled plots\n",
        "- Grid and legends for clarity\n",
        "\n",
        "### 2. Logging Setup\n",
        "```python\n",
        "def setup_logging(experiment_name):\n",
        "    \"\"\"Configures logging system\"\"\"\n",
        "```\n",
        "Features:\n",
        "- Rotating file handler (10MB limit)\n",
        "- Console output\n",
        "- Timestamp-based log files\n",
        "- Formatted log messages\n",
        "\n",
        "### 3. Training Pipeline\n",
        "```python\n",
        "def test_training_pipeline():\n",
        "    \"\"\"Main training execution function\"\"\"\n",
        "```\n",
        "\n",
        "#### Pipeline Steps:\n",
        "1. **Setup**:\n",
        "   - Create experiment directories\n",
        "   - Initialize logging\n",
        "   - Set up checkpointing\n",
        "\n",
        "2. **Data Preparation**:\n",
        "   - Train/validation split (80/20)\n",
        "   - Dataset creation\n",
        "   - Data validation checks\n",
        "\n",
        "3. **Training Execution**:\n",
        "   - Model initialization\n",
        "   - Training loop\n",
        "   - Progress monitoring\n",
        "   - Error handling\n",
        "\n",
        "4. **Results Processing**:\n",
        "   - Metrics logging\n",
        "   - CSV export\n",
        "   - Plot generation\n",
        "   - Memory usage tracking\n",
        "\n",
        "### Error Handling:\n",
        "```python\n",
        "try:\n",
        "    history, trainer = test_training_pipeline()\n",
        "except Exception as e:\n",
        "    # Comprehensive error logging\n",
        "    # State saving for debugging\n",
        "```\n",
        "\n",
        "## Usage:\n",
        "```python\n",
        "if __name__ == \"__main__\":\n",
        "    torch.manual_seed(42)  # Reproducibility\n",
        "    history, trainer = test_training_pipeline()\n",
        "```\n",
        "\n",
        "## Output Files:\n",
        "1. Training logs (`logs/experiment_name_timestamp.log`)\n",
        "2. Checkpoints (`DETR_CHECKPOINTS/experiment_name/`)\n",
        "3. Metrics CSV (`metrics.csv`)\n",
        "4. Training plots (`training_history.png`)\n",
        "5. Error states (if any) (`error_state.pth`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "DgfUPy2iZUPq",
        "outputId": "80887702-3028-4b2b-dcdd-bdfc1eca9c4a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-30 04:02:54 - INFO - Starting training pipeline\n",
            "2024-10-30 04:02:54 - INFO - ==================================================\n",
            "2024-10-30 04:02:54 - INFO - GPU: NVIDIA GeForce RTX 4090\n",
            "2024-10-30 04:02:54 - INFO - Initial GPU Memory: 196.52MB\n",
            "2024-10-30 04:02:54 - INFO - Dataset Information:\n",
            "2024-10-30 04:02:54 - INFO - --------------------------------------------------\n",
            "2024-10-30 04:02:54 - INFO - Training data shape: (72584, 6)\n",
            "2024-10-30 04:02:54 - INFO - Validation data shape: (18147, 6)\n",
            "2024-10-30 04:02:54 - INFO - --------------------------------------------------\n",
            "2024-10-30 04:02:54 - INFO - \n",
            "Model Information:\n",
            "2024-10-30 04:02:54 - INFO - Total parameters: 43,156,043\n",
            "2024-10-30 04:02:54 - INFO - Trainable parameters: 42,933,643\n",
            "2024-10-30 04:02:54 - INFO - GPU Memory After Cache Clear: 196.52MB\n",
            "2024-10-30 04:02:54 - INFO - \n",
            "Training Setup:\n",
            "2024-10-30 04:02:54 - INFO - Batch Size: 4\n",
            "2024-10-30 04:02:54 - INFO - Learning Rate: 0.0002\n",
            "2024-10-30 04:02:54 - INFO - Number of Epochs: 5\n",
            "2024-10-30 04:02:54 - INFO - Training Steps per Epoch: 18146\n",
            "2024-10-30 04:02:54 - INFO - Total Training Steps: 90730\n",
            "2024-10-30 04:02:54 - INFO - Warmup Steps: 9073\n",
            "2024-10-30 04:02:54 - INFO - \n",
            "Starting training...\n",
            "2024-10-30 04:02:54 - INFO - ==================================================\n",
            "2024-10-30 04:02:54 - INFO - \n",
            "Epoch 1/5\n",
            "Training:   0%|          | 0/18146 [00:00<?, ?it/s]2024-10-30 04:02:55 - INFO - \n",
            "Step 0 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:02:55 - INFO - Category pred mean: 0.567, target mean: 0.033\n",
            "Training:   3%|▎         | 500/18146 [03:32<2:06:07,  2.33it/s, total=4.369, raw_losses=c:0.828/a:0.712/s:3.779, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.028(0.48)/a:0.053/s:0.139]2024-10-30 04:06:27 - INFO - \n",
            "Step 500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:06:27 - INFO - Category pred mean: 0.517, target mean: 0.033\n",
            "Training:   6%|▌         | 1000/18146 [07:04<2:00:58,  2.36it/s, total=3.274, raw_losses=c:0.782/a:0.689/s:2.137, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.069(0.46)/a:0.046/s:0.126]2024-10-30 04:09:59 - INFO - \n",
            "Step 1000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:09:59 - INFO - Category pred mean: 0.517, target mean: 0.033\n",
            "Training:   8%|▊         | 1500/18146 [10:36<1:57:27,  2.36it/s, total=2.238, raw_losses=c:0.754/a:0.638/s:0.588, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.067(0.42)/a:0.100/s:0.026]2024-10-30 04:13:30 - INFO - \n",
            "Step 1500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:13:30 - INFO - Category pred mean: 0.400, target mean: 0.033\n",
            "Training:  11%|█         | 2000/18146 [14:06<1:54:25,  2.35it/s, total=1.764, raw_losses=c:0.671/a:0.474/s:0.303, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.127(0.27)/a:0.199/s:0.023]2024-10-30 04:17:00 - INFO - \n",
            "Step 2000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:17:00 - INFO - Category pred mean: 0.325, target mean: 0.033\n",
            "Training:  14%|█▍        | 2500/18146 [17:36<1:51:38,  2.34it/s, total=1.507, raw_losses=c:0.614/a:0.274/s:0.363, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.143(0.17)/a:0.360/s:0.006]2024-10-30 04:20:30 - INFO - \n",
            "Step 2500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:20:30 - INFO - Category pred mean: 0.158, target mean: 0.033\n",
            "Training:  17%|█▋        | 3000/18146 [21:08<1:54:07,  2.21it/s, total=1.199, raw_losses=c:0.546/a:0.148/s:0.249, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.100(0.10)/a:0.514/s:0.003]2024-10-30 04:24:02 - INFO - \n",
            "Step 3000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:24:02 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  19%|█▉        | 3500/18146 [24:37<1:43:52,  2.35it/s, total=1.053, raw_losses=c:0.498/a:0.123/s:0.181, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.267(0.08)/a:0.515/s:0.008]2024-10-30 04:27:31 - INFO - \n",
            "Step 3500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:27:31 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  22%|██▏       | 4000/18146 [28:07<1:43:26,  2.28it/s, total=0.984, raw_losses=c:0.511/a:0.082/s:0.098, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.07)/a:0.410/s:0.001]2024-10-30 04:31:01 - INFO - \n",
            "Step 4000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:31:01 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  25%|██▍       | 4500/18146 [31:37<1:38:51,  2.30it/s, total=0.933, raw_losses=c:0.379/a:0.126/s:0.301, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.458(0.08)/a:0.449/s:0.251]2024-10-30 04:34:32 - INFO - \n",
            "Step 4500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:34:32 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  28%|██▊       | 5000/18146 [35:08<1:34:01,  2.33it/s, total=0.850, raw_losses=c:0.403/a:0.108/s:0.129, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.250(0.04)/a:0.394/s:0.001]2024-10-30 04:38:02 - INFO - \n",
            "Step 5000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:38:02 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  30%|███       | 5500/18146 [38:36<1:28:29,  2.38it/s, total=0.716, raw_losses=c:0.326/a:0.085/s:0.155, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.375(0.05)/a:0.383/s:0.000]2024-10-30 04:41:31 - INFO - \n",
            "Step 5500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:41:31 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  33%|███▎      | 6000/18146 [42:06<1:26:17,  2.35it/s, total=0.801, raw_losses=c:0.329/a:0.093/s:0.272, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.333(0.04)/a:0.161/s:0.000]2024-10-30 04:45:00 - INFO - \n",
            "Step 6000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:45:00 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  36%|███▌      | 6500/18146 [45:37<1:22:09,  2.36it/s, total=0.666, raw_losses=c:0.347/a:0.063/s:0.050, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.04)/a:0.458/s:0.250]2024-10-30 04:48:31 - INFO - \n",
            "Step 6500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:48:31 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  39%|███▊      | 7000/18146 [49:07<1:20:41,  2.30it/s, total=0.860, raw_losses=c:0.410/a:0.087/s:0.161, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.05)/a:0.549/s:0.250]2024-10-30 04:52:01 - INFO - \n",
            "Step 7000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:52:01 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  41%|████▏     | 7500/18146 [52:36<1:15:12,  2.36it/s, total=0.551, raw_losses=c:0.252/a:0.098/s:0.063, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.03)/a:0.432/s:0.251]2024-10-30 04:55:31 - INFO - \n",
            "Step 7500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:55:31 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  44%|████▍     | 8000/18146 [56:05<1:11:29,  2.37it/s, total=0.842, raw_losses=c:0.290/a:0.074/s:0.478, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.375(0.05)/a:0.208/s:0.278]2024-10-30 04:59:00 - INFO - \n",
            "Step 8000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 04:59:00 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  47%|████▋     | 8500/18146 [59:35<1:11:15,  2.26it/s, total=0.705, raw_losses=c:0.279/a:0.080/s:0.271, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.03)/a:0.338/s:0.000]2024-10-30 05:02:29 - INFO - \n",
            "Step 8500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:02:29 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  50%|████▉     | 9000/18146 [1:03:05<1:04:23,  2.37it/s, total=0.681, raw_losses=c:0.345/a:0.068/s:0.073, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.07)/a:0.429/s:0.750]2024-10-30 05:05:59 - INFO - \n",
            "Step 9000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:05:59 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  52%|█████▏    | 9500/18146 [1:06:36<1:01:11,  2.35it/s, total=0.727, raw_losses=c:0.363/a:0.070/s:0.097, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.03)/a:0.675/s:0.500]2024-10-30 05:09:31 - INFO - \n",
            "Step 9500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:09:31 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  55%|█████▌    | 10000/18146 [1:10:07<57:44,  2.35it/s, total=0.751, raw_losses=c:0.310/a:0.097/s:0.234, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.05)/a:0.296/s:0.008] 2024-10-30 05:13:01 - INFO - \n",
            "Step 10000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:13:01 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  58%|█████▊    | 10500/18146 [1:13:34<54:20,  2.35it/s, total=0.760, raw_losses=c:0.247/a:0.093/s:0.429, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.03)/a:0.506/s:0.000]  2024-10-30 05:16:29 - INFO - \n",
            "Step 10500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:16:29 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  61%|██████    | 11000/18146 [1:17:04<49:56,  2.38it/s, total=0.761, raw_losses=c:0.397/a:0.099/s:0.013, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.05)/a:0.486/s:1.000]  2024-10-30 05:19:58 - INFO - \n",
            "Step 11000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:19:58 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  63%|██████▎   | 11500/18146 [1:20:33<46:29,  2.38it/s, total=0.854, raw_losses=c:0.373/a:0.093/s:0.242, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.500(0.04)/a:0.476/s:0.500]  2024-10-30 05:23:28 - INFO - \n",
            "Step 11500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:23:28 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  66%|██████▌   | 12000/18146 [1:24:01<43:23,  2.36it/s, total=0.628, raw_losses=c:0.245/a:0.103/s:0.200, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.583(0.04)/a:0.543/s:0.250]2024-10-30 05:26:55 - INFO - \n",
            "Step 12000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:26:55 - INFO - Category pred mean: 0.025, target mean: 0.033\n",
            "Training:  69%|██████▉   | 12500/18146 [1:27:31<39:37,  2.37it/s, total=0.601, raw_losses=c:0.275/a:0.100/s:0.079, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.06)/a:0.339/s:0.250]2024-10-30 05:30:26 - INFO - \n",
            "Step 12500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:30:26 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  72%|███████▏  | 13000/18146 [1:31:00<36:02,  2.38it/s, total=0.634, raw_losses=c:0.243/a:0.078/s:0.255, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.250(0.03)/a:0.553/s:0.250]2024-10-30 05:33:54 - INFO - \n",
            "Step 13000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:33:54 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  74%|███████▍  | 13500/18146 [1:34:27<32:43,  2.37it/s, total=0.623, raw_losses=c:0.284/a:0.089/s:0.108, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.04)/a:0.215/s:0.500]2024-10-30 05:37:22 - INFO - \n",
            "Step 13500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:37:22 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  77%|███████▋  | 14000/18146 [1:37:56<28:53,  2.39it/s, total=0.509, raw_losses=c:0.228/a:0.082/s:0.083, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.03)/a:0.392/s:0.750]2024-10-30 05:40:50 - INFO - \n",
            "Step 14000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:40:50 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  80%|███████▉  | 14500/18146 [1:41:25<25:55,  2.34it/s, total=0.711, raw_losses=c:0.354/a:0.098/s:0.048, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.04)/a:0.417/s:0.500]2024-10-30 05:44:20 - INFO - \n",
            "Step 14500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:44:20 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  83%|████████▎ | 15000/18146 [1:44:56<22:27,  2.33it/s, total=0.656, raw_losses=c:0.311/a:0.039/s:0.171, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.04)/a:0.369/s:0.500]2024-10-30 05:47:51 - INFO - \n",
            "Step 15000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:47:51 - INFO - Category pred mean: 0.025, target mean: 0.033\n",
            "Training:  85%|████████▌ | 15500/18146 [1:48:26<19:27,  2.27it/s, total=0.601, raw_losses=c:0.264/a:0.086/s:0.131, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.05)/a:0.392/s:0.500]2024-10-30 05:51:21 - INFO - \n",
            "Step 15500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:51:21 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  88%|████████▊ | 16000/18146 [1:51:53<15:13,  2.35it/s, total=0.535, raw_losses=c:0.250/a:0.116/s:0.014, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.04)/a:0.262/s:0.750]2024-10-30 05:54:48 - INFO - \n",
            "Step 16000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:54:48 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  91%|█████████ | 16500/18146 [1:55:22<11:32,  2.38it/s, total=0.523, raw_losses=c:0.259/a:0.066/s:0.048, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.03)/a:0.446/s:0.750]2024-10-30 05:58:17 - INFO - \n",
            "Step 16500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 05:58:17 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  94%|█████████▎| 17000/18146 [1:58:51<08:05,  2.36it/s, total=0.685, raw_losses=c:0.293/a:0.079/s:0.203, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.06)/a:0.590/s:0.250]2024-10-30 06:01:46 - INFO - \n",
            "Step 17000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 06:01:46 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  96%|█████████▋| 17500/18146 [2:02:20<04:36,  2.34it/s, total=0.553, raw_losses=c:0.250/a:0.092/s:0.078, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.04)/a:0.380/s:0.250]2024-10-30 06:05:15 - INFO - \n",
            "Step 17500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 06:05:15 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  99%|█████████▉| 18000/18146 [2:05:50<01:01,  2.36it/s, total=0.633, raw_losses=c:0.226/a:0.125/s:0.223, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.333(0.07)/a:0.381/s:0.000]2024-10-30 06:08:44 - INFO - \n",
            "Step 18000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 06:08:44 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training: 100%|██████████| 18146/18146 [2:06:51<00:00,  2.38it/s, total=0.763, raw_losses=c:0.307/a:0.059/s:0.327, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.333(0.06)/a:0.444/s:0.081]\n",
            "Validating: 100%|██████████| 4537/4537 [18:48<00:00,  4.02it/s, total=0.356, cat=0.175(1.65), attr=0.062(1.00), seg=0.008(0.61), metrics=cat: 0.556, cat: 0.044, cat: 0.033, att: 0.617, seg: 1.000]\n",
            "2024-10-30 06:28:34 - INFO - \n",
            "Training Metrics:\n",
            "2024-10-30 06:28:34 - INFO - Total Loss: 1.0546\n",
            "2024-10-30 06:28:34 - INFO - Task Losses:\n",
            "2024-10-30 06:28:34 - INFO -   category: 0.3856 (weight: 1.6487)\n",
            "2024-10-30 06:28:34 - INFO -   attribute: 0.1604 (weight: 1.0000)\n",
            "2024-10-30 06:28:34 - INFO -   segmentation: 0.4263 (weight: 0.6065)\n",
            "2024-10-30 06:28:34 - INFO - Performance Metrics:\n",
            "2024-10-30 06:28:34 - INFO -   category_dice: 0.2020\n",
            "2024-10-30 06:28:34 - INFO -   cat_pred_mean: 0.0971\n",
            "2024-10-30 06:28:34 - INFO -   cat_target_mean: 0.0333\n",
            "2024-10-30 06:28:34 - INFO -   attribute_dice: 0.3859\n",
            "2024-10-30 06:28:34 - INFO -   segmentation_miou: 0.2315\n",
            "2024-10-30 06:28:34 - INFO - Train Loss: 1.0546\n",
            "2024-10-30 06:28:34 - INFO - Train Metrics: {'category_dice': '0.2020', 'cat_pred_mean': '0.0971', 'cat_target_mean': '0.0333', 'attribute_dice': '0.3859', 'segmentation_miou': '0.2315', 'category_loss': '0.3856', 'attribute_loss': '0.1604', 'segmentation_loss': '0.4263', 'category_weight': '1.6487', 'attribute_weight': '1.0000', 'segmentation_weight': '0.6065'}\n",
            "2024-10-30 06:28:34 - INFO - Val Loss: 0.6094\n",
            "2024-10-30 06:28:34 - INFO - Val Metrics: {'category_dice': '0.3271', 'cat_pred_mean': '0.0485', 'cat_target_mean': '0.0333', 'attribute_dice': '0.4470', 'segmentation_miou': '0.4613', 'category_loss': '0.2601', 'attribute_loss': '0.0792', 'segmentation_loss': '0.1672', 'category_weight': '1.6487', 'attribute_weight': '1.0000', 'segmentation_weight': '0.6065'}\n",
            "2024-10-30 06:28:34 - INFO - Learning Rate: 0.000005\n",
            "2024-10-30 06:28:34 - INFO - Saved checkpoint: DETR_CHECKPOINTS/fashion_multitask_full/checkpoint_latest.pth\n",
            "2024-10-30 06:28:35 - INFO - Saved best model: DETR_CHECKPOINTS/fashion_multitask_full/model_best.pth\n",
            "2024-10-30 06:28:35 - INFO - \n",
            "Epoch 2/5\n",
            "Training:   0%|          | 0/18146 [00:00<?, ?it/s]2024-10-30 06:28:36 - INFO - \n",
            "Step 0 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 06:28:36 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:   3%|▎         | 500/18146 [03:30<2:07:56,  2.30it/s, total=0.546, raw_losses=c:0.221/a:0.076/s:0.173, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.250(0.06)/a:0.340/s:0.251]2024-10-30 06:32:06 - INFO - \n",
            "Step 500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 06:32:06 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:   6%|▌         | 1000/18146 [06:59<2:02:22,  2.34it/s, total=0.705, raw_losses=c:0.358/a:0.084/s:0.050, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.05)/a:0.360/s:0.750]2024-10-30 06:35:35 - INFO - \n",
            "Step 1000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 06:35:35 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:   8%|▊         | 1500/18146 [10:29<1:58:14,  2.35it/s, total=0.436, raw_losses=c:0.198/a:0.091/s:0.031, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.667(0.07)/a:0.342/s:0.750]2024-10-30 06:39:05 - INFO - \n",
            "Step 1500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 06:39:05 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  11%|█         | 2000/18146 [14:00<1:53:48,  2.36it/s, total=0.376, raw_losses=c:0.179/a:0.059/s:0.037, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.708(0.07)/a:0.298/s:0.750]2024-10-30 06:42:35 - INFO - \n",
            "Step 2000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 06:42:35 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  14%|█▍        | 2500/18146 [17:31<1:51:27,  2.34it/s, total=0.731, raw_losses=c:0.280/a:0.088/s:0.299, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.05)/a:0.529/s:0.250]2024-10-30 06:46:07 - INFO - \n",
            "Step 2500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 06:46:07 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  17%|█▋        | 3000/18146 [21:01<1:47:00,  2.36it/s, total=0.389, raw_losses=c:0.172/a:0.081/s:0.039, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.542(0.06)/a:0.395/s:0.750]2024-10-30 06:49:36 - INFO - \n",
            "Step 3000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 06:49:36 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  19%|█▉        | 3500/18146 [24:30<1:45:40,  2.31it/s, total=0.669, raw_losses=c:0.241/a:0.085/s:0.310, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.250(0.05)/a:0.459/s:0.000]2024-10-30 06:53:06 - INFO - \n",
            "Step 3500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 06:53:06 - INFO - Category pred mean: 0.025, target mean: 0.033\n",
            "Training:  22%|██▏       | 4000/18146 [28:00<1:41:56,  2.31it/s, total=0.496, raw_losses=c:0.169/a:0.078/s:0.230, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.667(0.07)/a:0.558/s:0.253]2024-10-30 06:56:36 - INFO - \n",
            "Step 4000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 06:56:36 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  25%|██▍       | 4500/18146 [31:30<1:36:23,  2.36it/s, total=0.737, raw_losses=c:0.283/a:0.083/s:0.308, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.03)/a:0.617/s:0.500]2024-10-30 07:00:05 - INFO - \n",
            "Step 4500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:00:05 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  28%|██▊       | 5000/18146 [34:59<1:34:29,  2.32it/s, total=0.519, raw_losses=c:0.254/a:0.079/s:0.035, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.03)/a:0.407/s:0.500]2024-10-30 07:03:35 - INFO - \n",
            "Step 5000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:03:35 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  30%|███       | 5500/18146 [38:28<1:29:27,  2.36it/s, total=0.478, raw_losses=c:0.188/a:0.058/s:0.183, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.04)/a:0.332/s:0.000]2024-10-30 07:07:04 - INFO - \n",
            "Step 5500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:07:04 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  33%|███▎      | 6000/18146 [41:57<1:25:31,  2.37it/s, total=0.505, raw_losses=c:0.208/a:0.068/s:0.155, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.517(0.08)/a:0.374/s:0.785]2024-10-30 07:10:33 - INFO - \n",
            "Step 6000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:10:33 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  36%|███▌      | 6500/18146 [45:27<1:23:26,  2.33it/s, total=0.535, raw_losses=c:0.204/a:0.097/s:0.168, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.04)/a:0.474/s:0.863]2024-10-30 07:14:03 - INFO - \n",
            "Step 6500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:14:03 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  39%|███▊      | 7000/18146 [48:56<1:18:16,  2.37it/s, total=1.121, raw_losses=c:0.323/a:0.079/s:0.840, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.05)/a:0.445/s:0.500]2024-10-30 07:17:32 - INFO - \n",
            "Step 7000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:17:32 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  41%|████▏     | 7500/18146 [52:26<1:15:40,  2.34it/s, total=0.606, raw_losses=c:0.274/a:0.094/s:0.100, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.04)/a:0.575/s:0.250]2024-10-30 07:21:02 - INFO - \n",
            "Step 7500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:21:02 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  44%|████▍     | 8000/18146 [55:55<1:11:32,  2.36it/s, total=0.541, raw_losses=c:0.229/a:0.056/s:0.178, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.04)/a:0.268/s:0.500]2024-10-30 07:24:31 - INFO - \n",
            "Step 8000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:24:31 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  47%|████▋     | 8500/18146 [59:24<1:07:43,  2.37it/s, total=0.542, raw_losses=c:0.271/a:0.056/s:0.065, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.04)/a:0.332/s:0.500]2024-10-30 07:28:00 - INFO - \n",
            "Step 8500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:28:00 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  50%|████▉     | 9000/18146 [1:02:54<1:06:54,  2.28it/s, total=0.649, raw_losses=c:0.243/a:0.085/s:0.269, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.333(0.08)/a:0.443/s:0.301]2024-10-30 07:31:30 - INFO - \n",
            "Step 9000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:31:30 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  52%|█████▏    | 9500/18146 [1:06:24<1:01:51,  2.33it/s, total=0.563, raw_losses=c:0.213/a:0.087/s:0.206, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.03)/a:0.338/s:0.250]2024-10-30 07:35:00 - INFO - \n",
            "Step 9500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:35:00 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  55%|█████▌    | 10000/18146 [1:09:54<58:53,  2.31it/s, total=0.710, raw_losses=c:0.342/a:0.099/s:0.079, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.06)/a:0.539/s:0.500] 2024-10-30 07:38:30 - INFO - \n",
            "Step 10000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:38:30 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  58%|█████▊    | 10500/18146 [1:13:23<53:58,  2.36it/s, total=0.490, raw_losses=c:0.227/a:0.075/s:0.067, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.500(0.04)/a:0.474/s:0.500]  2024-10-30 07:41:59 - INFO - \n",
            "Step 10500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:41:59 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  61%|██████    | 11000/18146 [1:16:53<50:48,  2.34it/s, total=0.408, raw_losses=c:0.191/a:0.052/s:0.067, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.04)/a:0.270/s:0.500]  2024-10-30 07:45:29 - INFO - \n",
            "Step 11000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:45:29 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  63%|██████▎   | 11500/18146 [1:20:22<49:11,  2.25it/s, total=0.395, raw_losses=c:0.202/a:0.053/s:0.015, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.05)/a:0.535/s:0.750]2024-10-30 07:48:58 - INFO - \n",
            "Step 11500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:48:58 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  66%|██████▌   | 12000/18146 [1:23:52<43:36,  2.35it/s, total=0.520, raw_losses=c:0.204/a:0.072/s:0.184, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.500(0.06)/a:0.476/s:0.500]2024-10-30 07:52:28 - INFO - \n",
            "Step 12000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:52:28 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  69%|██████▉   | 12500/18146 [1:27:22<39:44,  2.37it/s, total=0.688, raw_losses=c:0.301/a:0.100/s:0.153, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.03)/a:0.470/s:0.500]2024-10-30 07:55:57 - INFO - \n",
            "Step 12500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:55:57 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  72%|███████▏  | 13000/18146 [1:30:53<38:04,  2.25it/s, total=0.438, raw_losses=c:0.163/a:0.099/s:0.118, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.708(0.07)/a:0.445/s:0.500]2024-10-30 07:59:29 - INFO - \n",
            "Step 13000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 07:59:29 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  74%|███████▍  | 13500/18146 [1:34:23<32:57,  2.35it/s, total=0.732, raw_losses=c:0.233/a:0.055/s:0.482, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.333(0.05)/a:0.663/s:0.309]2024-10-30 08:02:59 - INFO - \n",
            "Step 13500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 08:02:59 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  77%|███████▋  | 14000/18146 [1:37:55<29:31,  2.34it/s, total=0.560, raw_losses=c:0.278/a:0.099/s:0.003, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.125(0.08)/a:0.492/s:1.000]2024-10-30 08:06:30 - INFO - \n",
            "Step 14000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 08:06:30 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  80%|███████▉  | 14500/18146 [1:41:25<25:49,  2.35it/s, total=0.519, raw_losses=c:0.208/a:0.086/s:0.149, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.500(0.05)/a:0.331/s:0.500]2024-10-30 08:10:01 - INFO - \n",
            "Step 14500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 08:10:01 - INFO - Category pred mean: 0.025, target mean: 0.033\n",
            "Training:  83%|████████▎ | 15000/18146 [1:44:56<22:08,  2.37it/s, total=0.411, raw_losses=c:0.134/a:0.100/s:0.148, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.875(0.05)/a:0.567/s:0.500]2024-10-30 08:13:31 - INFO - \n",
            "Step 15000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 08:13:31 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  85%|████████▌ | 15500/18146 [1:48:26<19:03,  2.31it/s, total=0.762, raw_losses=c:0.306/a:0.085/s:0.284, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.04)/a:0.570/s:0.500]2024-10-30 08:17:02 - INFO - \n",
            "Step 15500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 08:17:02 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  88%|████████▊ | 16000/18146 [1:51:56<15:40,  2.28it/s, total=0.603, raw_losses=c:0.265/a:0.100/s:0.109, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.04)/a:0.389/s:0.500]2024-10-30 08:20:31 - INFO - \n",
            "Step 16000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 08:20:31 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  91%|█████████ | 16500/18146 [1:55:26<12:00,  2.28it/s, total=0.721, raw_losses=c:0.377/a:0.075/s:0.039, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.250(0.06)/a:0.334/s:0.750]2024-10-30 08:24:01 - INFO - \n",
            "Step 16500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 08:24:01 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  94%|█████████▎| 17000/18146 [1:58:59<08:03,  2.37it/s, total=0.623, raw_losses=c:0.302/a:0.058/s:0.110, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.350(0.08)/a:0.686/s:0.500]2024-10-30 08:27:35 - INFO - \n",
            "Step 17000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 08:27:35 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  96%|█████████▋| 17500/18146 [2:02:28<04:35,  2.34it/s, total=0.447, raw_losses=c:0.174/a:0.079/s:0.134, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.05)/a:0.353/s:0.500]2024-10-30 08:31:03 - INFO - \n",
            "Step 17500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 08:31:03 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  99%|█████████▉| 18000/18146 [2:05:57<01:02,  2.33it/s, total=0.594, raw_losses=c:0.268/a:0.076/s:0.126, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.03)/a:0.526/s:0.500]2024-10-30 08:34:33 - INFO - \n",
            "Step 18000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 08:34:33 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training: 100%|██████████| 18146/18146 [2:06:58<00:00,  2.38it/s, total=0.455, raw_losses=c:0.179/a:0.074/s:0.142, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.542(0.06)/a:0.531/s:0.750]\n",
            "Validating: 100%|██████████| 4537/4537 [18:29<00:00,  4.09it/s, total=0.219, cat=0.103(1.65), attr=0.048(1.00), seg=0.003(0.61), metrics=cat: 0.889, cat: 0.044, cat: 0.033, att: 0.694, seg: 1.000]\n",
            "2024-10-30 08:54:03 - INFO - \n",
            "Training Metrics:\n",
            "2024-10-30 08:54:03 - INFO - Total Loss: 0.5882\n",
            "2024-10-30 08:54:03 - INFO - Task Losses:\n",
            "2024-10-30 08:54:03 - INFO -   category: 0.2532 (weight: 1.6487)\n",
            "2024-10-30 08:54:03 - INFO -   attribute: 0.0757 (weight: 1.0000)\n",
            "2024-10-30 08:54:03 - INFO -   segmentation: 0.1567 (weight: 0.6065)\n",
            "2024-10-30 08:54:03 - INFO - Performance Metrics:\n",
            "2024-10-30 08:54:03 - INFO -   category_dice: 0.3302\n",
            "2024-10-30 08:54:03 - INFO -   cat_pred_mean: 0.0458\n",
            "2024-10-30 08:54:03 - INFO -   cat_target_mean: 0.0333\n",
            "2024-10-30 08:54:03 - INFO -   attribute_dice: 0.4593\n",
            "2024-10-30 08:54:03 - INFO -   segmentation_miou: 0.4769\n",
            "2024-10-30 08:54:03 - INFO - Train Loss: 0.5882\n",
            "2024-10-30 08:54:03 - INFO - Train Metrics: {'category_dice': '0.3302', 'cat_pred_mean': '0.0458', 'cat_target_mean': '0.0333', 'attribute_dice': '0.4593', 'segmentation_miou': '0.4769', 'category_loss': '0.2532', 'attribute_loss': '0.0757', 'segmentation_loss': '0.1567', 'category_weight': '1.6487', 'attribute_weight': '1.0000', 'segmentation_weight': '0.6065'}\n",
            "2024-10-30 08:54:03 - INFO - Val Loss: 0.5328\n",
            "2024-10-30 08:54:03 - INFO - Val Metrics: {'category_dice': '0.4224', 'cat_pred_mean': '0.0571', 'cat_target_mean': '0.0333', 'attribute_dice': '0.4721', 'segmentation_miou': '0.4797', 'category_loss': '0.2244', 'attribute_loss': '0.0725', 'segmentation_loss': '0.1492', 'category_weight': '1.6487', 'attribute_weight': '1.0000', 'segmentation_weight': '0.6065'}\n",
            "2024-10-30 08:54:03 - INFO - Learning Rate: 0.000010\n",
            "2024-10-30 08:54:04 - INFO - Saved checkpoint: DETR_CHECKPOINTS/fashion_multitask_full/checkpoint_latest.pth\n",
            "2024-10-30 08:54:05 - INFO - Saved best model: DETR_CHECKPOINTS/fashion_multitask_full/model_best.pth\n",
            "2024-10-30 08:54:05 - INFO - \n",
            "Epoch 3/5\n",
            "Training:   0%|          | 0/18146 [00:00<?, ?it/s]2024-10-30 08:54:06 - INFO - \n",
            "Step 0 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 08:54:06 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:   3%|▎         | 500/18146 [03:28<2:20:28,  2.09it/s, total=0.894, raw_losses=c:0.463/a:0.093/s:0.063, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.06)/a:0.240/s:0.500]2024-10-30 08:57:34 - INFO - \n",
            "Step 500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 08:57:34 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:   6%|▌         | 1000/18146 [06:57<2:02:31,  2.33it/s, total=0.660, raw_losses=c:0.278/a:0.068/s:0.220, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.333(0.05)/a:0.585/s:0.000]2024-10-30 09:01:03 - INFO - \n",
            "Step 1000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:01:03 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:   8%|▊         | 1500/18146 [10:26<1:56:57,  2.37it/s, total=0.578, raw_losses=c:0.194/a:0.058/s:0.329, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.500(0.04)/a:0.636/s:0.021]2024-10-30 09:04:32 - INFO - \n",
            "Step 1500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:04:32 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  11%|█         | 2000/18146 [13:54<1:52:22,  2.39it/s, total=0.435, raw_losses=c:0.220/a:0.048/s:0.040, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.03)/a:0.375/s:0.750]2024-10-30 09:07:59 - INFO - \n",
            "Step 2000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:07:59 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  14%|█▍        | 2500/18146 [17:21<1:50:05,  2.37it/s, total=0.705, raw_losses=c:0.262/a:0.077/s:0.324, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.250(0.03)/a:0.380/s:0.000]2024-10-30 09:11:27 - INFO - \n",
            "Step 2500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:11:27 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  17%|█▋        | 3000/18146 [20:51<1:46:07,  2.38it/s, total=0.612, raw_losses=c:0.273/a:0.079/s:0.137, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.250(0.05)/a:0.518/s:0.251]2024-10-30 09:14:56 - INFO - \n",
            "Step 3000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:14:56 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  19%|█▉        | 3500/18146 [24:19<1:42:54,  2.37it/s, total=0.737, raw_losses=c:0.357/a:0.098/s:0.082, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.03)/a:0.469/s:0.250]2024-10-30 09:18:25 - INFO - \n",
            "Step 3500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:18:25 - INFO - Category pred mean: 0.017, target mean: 0.033\n",
            "Training:  22%|██▏       | 4000/18146 [27:48<1:39:36,  2.37it/s, total=0.442, raw_losses=c:0.235/a:0.052/s:0.005, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.04)/a:0.310/s:0.750]2024-10-30 09:21:54 - INFO - \n",
            "Step 4000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:21:54 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  25%|██▍       | 4500/18146 [31:17<1:36:56,  2.35it/s, total=0.450, raw_losses=c:0.211/a:0.060/s:0.070, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.250(0.05)/a:0.420/s:0.750]2024-10-30 09:25:23 - INFO - \n",
            "Step 4500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:25:23 - INFO - Category pred mean: 0.075, target mean: 0.033\n",
            "Training:  28%|██▊       | 5000/18146 [34:46<1:33:19,  2.35it/s, total=0.661, raw_losses=c:0.352/a:0.050/s:0.050, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.292(0.06)/a:0.580/s:0.500]2024-10-30 09:28:52 - INFO - \n",
            "Step 5000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:28:52 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  30%|███       | 5500/18146 [38:14<1:28:42,  2.38it/s, total=0.520, raw_losses=c:0.258/a:0.072/s:0.036, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.06)/a:0.622/s:0.750]2024-10-30 09:32:20 - INFO - \n",
            "Step 5500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:32:20 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  33%|███▎      | 6000/18146 [41:45<1:25:04,  2.38it/s, total=0.587, raw_losses=c:0.236/a:0.061/s:0.225, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.583(0.06)/a:0.531/s:0.253]2024-10-30 09:35:51 - INFO - \n",
            "Step 6000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:35:51 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  36%|███▌      | 6500/18146 [45:14<1:20:51,  2.40it/s, total=0.758, raw_losses=c:0.382/a:0.084/s:0.073, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.05)/a:0.312/s:0.250]2024-10-30 09:39:20 - INFO - \n",
            "Step 6500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:39:20 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  39%|███▊      | 7000/18146 [48:42<1:18:17,  2.37it/s, total=0.533, raw_losses=c:0.236/a:0.079/s:0.109, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.333(0.06)/a:0.437/s:0.750]2024-10-30 09:42:48 - INFO - \n",
            "Step 7000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:42:48 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  41%|████▏     | 7500/18146 [52:10<1:14:47,  2.37it/s, total=0.694, raw_losses=c:0.234/a:0.109/s:0.331, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.667(0.05)/a:0.441/s:0.554]2024-10-30 09:46:16 - INFO - \n",
            "Step 7500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:46:16 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  44%|████▍     | 8000/18146 [55:38<1:10:39,  2.39it/s, total=0.676, raw_losses=c:0.353/a:0.069/s:0.042, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.04)/a:0.179/s:0.750]2024-10-30 09:49:44 - INFO - \n",
            "Step 8000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:49:44 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  47%|████▋     | 8500/18146 [59:06<1:08:16,  2.35it/s, total=0.542, raw_losses=c:0.240/a:0.077/s:0.115, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.500(0.05)/a:0.517/s:0.282]2024-10-30 09:53:12 - INFO - \n",
            "Step 8500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:53:12 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  50%|████▉     | 9000/18146 [1:02:34<1:03:47,  2.39it/s, total=0.513, raw_losses=c:0.195/a:0.068/s:0.204, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.07)/a:0.654/s:0.250]2024-10-30 09:56:40 - INFO - \n",
            "Step 9000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 09:56:40 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  52%|█████▏    | 9500/18146 [1:06:02<1:01:21,  2.35it/s, total=0.611, raw_losses=c:0.238/a:0.081/s:0.228, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.06)/a:0.617/s:0.387]2024-10-30 10:00:08 - INFO - \n",
            "Step 9500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:00:08 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  55%|█████▌    | 10000/18146 [1:09:30<57:14,  2.37it/s, total=0.736, raw_losses=c:0.311/a:0.081/s:0.236, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.125(0.06)/a:0.332/s:0.250] 2024-10-30 10:03:36 - INFO - \n",
            "Step 10000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:03:36 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  58%|█████▊    | 10500/18146 [1:12:59<53:33,  2.38it/s, total=0.748, raw_losses=c:0.334/a:0.083/s:0.188, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.292(0.06)/a:0.592/s:0.500]  2024-10-30 10:07:05 - INFO - \n",
            "Step 10500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:07:05 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  61%|██████    | 11000/18146 [1:16:26<50:11,  2.37it/s, total=0.573, raw_losses=c:0.243/a:0.083/s:0.145, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.542(0.08)/a:0.540/s:0.500]2024-10-30 10:10:32 - INFO - \n",
            "Step 11000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:10:32 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  63%|██████▎   | 11500/18146 [1:19:54<46:52,  2.36it/s, total=0.570, raw_losses=c:0.168/a:0.092/s:0.331, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.03)/a:0.557/s:0.500]2024-10-30 10:14:00 - INFO - \n",
            "Step 11500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:14:00 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  66%|██████▌   | 12000/18146 [1:23:23<43:45,  2.34it/s, total=0.627, raw_losses=c:0.303/a:0.074/s:0.088, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.06)/a:0.435/s:0.500]2024-10-30 10:17:29 - INFO - \n",
            "Step 12000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:17:29 - INFO - Category pred mean: 0.083, target mean: 0.033\n",
            "Training:  69%|██████▉   | 12500/18146 [1:26:52<39:23,  2.39it/s, total=0.546, raw_losses=c:0.278/a:0.087/s:0.001, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.07)/a:0.358/s:1.000]2024-10-30 10:20:58 - INFO - \n",
            "Step 12500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:20:58 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  72%|███████▏  | 13000/18146 [1:30:20<36:04,  2.38it/s, total=0.473, raw_losses=c:0.170/a:0.078/s:0.192, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.583(0.06)/a:0.380/s:0.250]2024-10-30 10:24:26 - INFO - \n",
            "Step 13000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:24:26 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  74%|███████▍  | 13500/18146 [1:33:47<32:27,  2.39it/s, total=0.433, raw_losses=c:0.183/a:0.098/s:0.054, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.03)/a:0.287/s:0.750]2024-10-30 10:27:53 - INFO - \n",
            "Step 13500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:27:53 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  77%|███████▋  | 14000/18146 [1:37:17<29:07,  2.37it/s, total=0.487, raw_losses=c:0.165/a:0.081/s:0.222, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.600(0.07)/a:0.567/s:0.011]2024-10-30 10:31:23 - INFO - \n",
            "Step 14000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:31:23 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  80%|███████▉  | 14500/18146 [1:40:47<25:47,  2.36it/s, total=0.808, raw_losses=c:0.384/a:0.054/s:0.200, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.06)/a:0.537/s:0.516]2024-10-30 10:34:53 - INFO - \n",
            "Step 14500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:34:53 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  83%|████████▎ | 15000/18146 [1:44:16<22:16,  2.35it/s, total=0.423, raw_losses=c:0.208/a:0.059/s:0.036, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.292(0.05)/a:0.459/s:0.750]2024-10-30 10:38:22 - INFO - \n",
            "Step 15000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:38:22 - INFO - Category pred mean: 0.075, target mean: 0.033\n",
            "Training:  85%|████████▌ | 15500/18146 [1:47:45<18:29,  2.38it/s, total=0.548, raw_losses=c:0.226/a:0.061/s:0.190, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.06)/a:0.325/s:0.250]2024-10-30 10:41:51 - INFO - \n",
            "Step 15500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:41:51 - INFO - Category pred mean: 0.075, target mean: 0.033\n",
            "Training:  88%|████████▊ | 16000/18146 [1:51:13<15:03,  2.38it/s, total=0.398, raw_losses=c:0.186/a:0.060/s:0.050, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.500(0.06)/a:0.658/s:0.750]2024-10-30 10:45:19 - INFO - \n",
            "Step 16000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:45:19 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  91%|█████████ | 16500/18146 [1:54:41<11:36,  2.36it/s, total=0.357, raw_losses=c:0.150/a:0.060/s:0.081, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.625(0.06)/a:0.629/s:0.521]2024-10-30 10:48:47 - INFO - \n",
            "Step 16500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:48:47 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  94%|█████████▎| 17000/18146 [1:58:09<08:05,  2.36it/s, total=0.419, raw_losses=c:0.204/a:0.065/s:0.028, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.458(0.09)/a:0.609/s:0.750]2024-10-30 10:52:14 - INFO - \n",
            "Step 17000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:52:14 - INFO - Category pred mean: 0.075, target mean: 0.033\n",
            "Training:  96%|█████████▋| 17500/18146 [2:01:38<04:51,  2.21it/s, total=0.394, raw_losses=c:0.148/a:0.071/s:0.132, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.667(0.05)/a:0.541/s:0.500]2024-10-30 10:55:43 - INFO - \n",
            "Step 17500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:55:43 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  99%|█████████▉| 18000/18146 [2:05:06<01:02,  2.32it/s, total=0.659, raw_losses=c:0.324/a:0.055/s:0.116, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.06)/a:0.312/s:0.250]2024-10-30 10:59:12 - INFO - \n",
            "Step 18000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 10:59:12 - INFO - Category pred mean: 0.075, target mean: 0.033\n",
            "Training: 100%|██████████| 18146/18146 [2:06:08<00:00,  2.40it/s, total=0.439, raw_losses=c:0.175/a:0.064/s:0.143, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.667(0.05)/a:0.696/s:0.500]\n",
            "Validating: 100%|██████████| 4537/4537 [18:34<00:00,  4.07it/s, total=0.176, cat=0.081(1.65), attr=0.043(1.00), seg=0.001(0.61), metrics=cat: 0.889, cat: 0.044, cat: 0.033, att: 0.694, seg: 1.000]\n",
            "2024-10-30 11:18:48 - INFO - \n",
            "Training Metrics:\n",
            "2024-10-30 11:18:48 - INFO - Total Loss: 0.5394\n",
            "2024-10-30 11:18:48 - INFO - Task Losses:\n",
            "2024-10-30 11:18:48 - INFO -   category: 0.2321 (weight: 1.6487)\n",
            "2024-10-30 11:18:48 - INFO -   attribute: 0.0717 (weight: 1.0000)\n",
            "2024-10-30 11:18:48 - INFO -   segmentation: 0.1404 (weight: 0.6065)\n",
            "2024-10-30 11:18:48 - INFO - Performance Metrics:\n",
            "2024-10-30 11:18:48 - INFO -   category_dice: 0.3980\n",
            "2024-10-30 11:18:48 - INFO -   cat_pred_mean: 0.0525\n",
            "2024-10-30 11:18:48 - INFO -   cat_target_mean: 0.0333\n",
            "2024-10-30 11:18:48 - INFO -   attribute_dice: 0.4720\n",
            "2024-10-30 11:18:48 - INFO -   segmentation_miou: 0.4878\n",
            "2024-10-30 11:18:48 - INFO - Train Loss: 0.5394\n",
            "2024-10-30 11:18:48 - INFO - Train Metrics: {'category_dice': '0.3980', 'cat_pred_mean': '0.0525', 'cat_target_mean': '0.0333', 'attribute_dice': '0.4720', 'segmentation_miou': '0.4878', 'category_loss': '0.2321', 'attribute_loss': '0.0717', 'segmentation_loss': '0.1404', 'category_weight': '1.6487', 'attribute_weight': '1.0000', 'segmentation_weight': '0.6065'}\n",
            "2024-10-30 11:18:48 - INFO - Val Loss: 0.4987\n",
            "2024-10-30 11:18:48 - INFO - Val Metrics: {'category_dice': '0.4604', 'cat_pred_mean': '0.0721', 'cat_target_mean': '0.0333', 'attribute_dice': '0.4852', 'segmentation_miou': '0.4867', 'category_loss': '0.2101', 'attribute_loss': '0.0702', 'segmentation_loss': '0.1353', 'category_weight': '1.6487', 'attribute_weight': '1.0000', 'segmentation_weight': '0.6065'}\n",
            "2024-10-30 11:18:48 - INFO - Learning Rate: 0.000009\n",
            "2024-10-30 11:18:49 - INFO - Saved checkpoint: DETR_CHECKPOINTS/fashion_multitask_full/checkpoint_latest.pth\n",
            "2024-10-30 11:18:50 - INFO - Saved best model: DETR_CHECKPOINTS/fashion_multitask_full/model_best.pth\n",
            "2024-10-30 11:18:50 - INFO - \n",
            "Epoch 4/5\n",
            "Training:   0%|          | 0/18146 [00:00<?, ?it/s]2024-10-30 11:18:51 - INFO - \n",
            "Step 0 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 11:18:51 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:   3%|▎         | 500/18146 [03:28<2:04:19,  2.37it/s, total=0.698, raw_losses=c:0.353/a:0.090/s:0.042, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.04)/a:0.418/s:0.500]2024-10-30 11:22:19 - INFO - \n",
            "Step 500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 11:22:19 - INFO - Category pred mean: 0.025, target mean: 0.033\n",
            "Training:   6%|▌         | 1000/18146 [06:58<2:01:02,  2.36it/s, total=0.559, raw_losses=c:0.226/a:0.088/s:0.163, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.05)/a:0.421/s:0.250]2024-10-30 11:25:49 - INFO - \n",
            "Step 1000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 11:25:49 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:   8%|▊         | 1500/18146 [10:27<1:57:31,  2.36it/s, total=0.348, raw_losses=c:0.141/a:0.056/s:0.098, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.667(0.05)/a:0.332/s:0.501]2024-10-30 11:29:18 - INFO - \n",
            "Step 1500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 11:29:18 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  11%|█         | 2000/18146 [13:55<1:59:34,  2.25it/s, total=0.326, raw_losses=c:0.164/a:0.053/s:0.004, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.542(0.05)/a:0.258/s:1.000]2024-10-30 11:32:46 - INFO - \n",
            "Step 2000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 11:32:46 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  14%|█▍        | 2500/18146 [17:22<1:48:46,  2.40it/s, total=0.419, raw_losses=c:0.184/a:0.084/s:0.052, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.500(0.06)/a:0.505/s:0.500]2024-10-30 11:36:12 - INFO - \n",
            "Step 2500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 11:36:12 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  17%|█▋        | 3000/18146 [20:48<1:45:43,  2.39it/s, total=0.528, raw_losses=c:0.241/a:0.051/s:0.130, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.600(0.07)/a:0.446/s:0.750]2024-10-30 11:39:39 - INFO - \n",
            "Step 3000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 11:39:39 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  19%|█▉        | 3500/18146 [24:17<1:45:49,  2.31it/s, total=0.473, raw_losses=c:0.183/a:0.089/s:0.135, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.542(0.06)/a:0.495/s:0.502]2024-10-30 11:43:08 - INFO - \n",
            "Step 3500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 11:43:08 - INFO - Category pred mean: 0.075, target mean: 0.033\n",
            "Training:  22%|██▏       | 4000/18146 [27:44<1:38:53,  2.38it/s, total=0.411, raw_losses=c:0.186/a:0.064/s:0.067, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.375(0.05)/a:0.479/s:0.250]2024-10-30 11:46:35 - INFO - \n",
            "Step 4000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 11:46:35 - INFO - Category pred mean: 0.075, target mean: 0.033\n",
            "Training:  25%|██▍       | 4500/18146 [31:14<1:35:57,  2.37it/s, total=0.736, raw_losses=c:0.318/a:0.072/s:0.230, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.05)/a:0.614/s:0.500]2024-10-30 11:50:04 - INFO - \n",
            "Step 4500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 11:50:04 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  28%|██▊       | 5000/18146 [34:40<1:32:45,  2.36it/s, total=0.624, raw_losses=c:0.281/a:0.055/s:0.175, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.125(0.07)/a:0.379/s:0.640]2024-10-30 11:53:31 - INFO - \n",
            "Step 5000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 11:53:31 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  30%|███       | 5500/18146 [38:09<1:28:51,  2.37it/s, total=0.618, raw_losses=c:0.288/a:0.079/s:0.105, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.06)/a:0.313/s:0.250]2024-10-30 11:57:00 - INFO - \n",
            "Step 5500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 11:57:00 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  33%|███▎      | 6000/18146 [41:38<1:25:43,  2.36it/s, total=0.444, raw_losses=c:0.144/a:0.065/s:0.233, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.750(0.06)/a:0.630/s:0.265]2024-10-30 12:00:29 - INFO - \n",
            "Step 6000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:00:29 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  36%|███▌      | 6500/18146 [45:07<1:21:25,  2.38it/s, total=0.534, raw_losses=c:0.220/a:0.067/s:0.171, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.250(0.03)/a:0.656/s:0.000]2024-10-30 12:03:58 - INFO - \n",
            "Step 6500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:03:58 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  39%|███▊      | 7000/18146 [48:35<1:17:52,  2.39it/s, total=0.479, raw_losses=c:0.206/a:0.082/s:0.095, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.333(0.05)/a:0.304/s:0.250]2024-10-30 12:07:26 - INFO - \n",
            "Step 7000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:07:26 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  41%|████▏     | 7500/18146 [52:02<1:15:07,  2.36it/s, total=0.382, raw_losses=c:0.173/a:0.065/s:0.052, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.500(0.06)/a:0.510/s:0.500]2024-10-30 12:10:53 - INFO - \n",
            "Step 7500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:10:53 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  44%|████▍     | 8000/18146 [55:30<1:11:03,  2.38it/s, total=0.401, raw_losses=c:0.177/a:0.051/s:0.097, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.500(0.07)/a:0.524/s:0.500]2024-10-30 12:14:21 - INFO - \n",
            "Step 8000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:14:21 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  47%|████▋     | 8500/18146 [58:57<1:07:33,  2.38it/s, total=0.397, raw_losses=c:0.149/a:0.086/s:0.107, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.05)/a:0.506/s:0.500]2024-10-30 12:17:48 - INFO - \n",
            "Step 8500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:17:48 - INFO - Category pred mean: 0.083, target mean: 0.033\n",
            "Training:  50%|████▉     | 9000/18146 [1:02:24<1:03:21,  2.41it/s, total=0.419, raw_losses=c:0.179/a:0.074/s:0.084, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.750(0.03)/a:0.457/s:0.500]2024-10-30 12:21:15 - INFO - \n",
            "Step 9000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:21:15 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  52%|█████▏    | 9500/18146 [1:05:52<1:00:19,  2.39it/s, total=0.530, raw_losses=c:0.229/a:0.059/s:0.155, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.05)/a:0.317/s:0.508]2024-10-30 12:24:43 - INFO - \n",
            "Step 9500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:24:43 - INFO - Category pred mean: 0.075, target mean: 0.033\n",
            "Training:  55%|█████▌    | 10000/18146 [1:09:20<57:15,  2.37it/s, total=0.377, raw_losses=c:0.134/a:0.056/s:0.164, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.708(0.07)/a:0.737/s:0.897] 2024-10-30 12:28:11 - INFO - \n",
            "Step 10000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:28:11 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  58%|█████▊    | 10500/18146 [1:12:46<52:59,  2.40it/s, total=0.472, raw_losses=c:0.203/a:0.055/s:0.134, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.250(0.03)/a:0.683/s:0.250]  2024-10-30 12:31:37 - INFO - \n",
            "Step 10500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:31:37 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  61%|██████    | 11000/18146 [1:16:15<50:05,  2.38it/s, total=0.419, raw_losses=c:0.182/a:0.072/s:0.077, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.583(0.07)/a:0.478/s:0.500]2024-10-30 12:35:06 - INFO - \n",
            "Step 11000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:35:06 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  63%|██████▎   | 11500/18146 [1:19:43<46:06,  2.40it/s, total=0.526, raw_losses=c:0.212/a:0.074/s:0.169, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.03)/a:0.309/s:0.250]2024-10-30 12:38:34 - INFO - \n",
            "Step 11500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:38:34 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  66%|██████▌   | 12000/18146 [1:23:11<43:38,  2.35it/s, total=0.517, raw_losses=c:0.236/a:0.068/s:0.100, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.250(0.06)/a:0.492/s:0.500]  2024-10-30 12:42:02 - INFO - \n",
            "Step 12000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:42:02 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  69%|██████▉   | 12500/18146 [1:26:39<39:25,  2.39it/s, total=0.510, raw_losses=c:0.264/a:0.061/s:0.023, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.05)/a:0.520/s:0.750]2024-10-30 12:45:30 - INFO - \n",
            "Step 12500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:45:30 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  72%|███████▏  | 13000/18146 [1:30:08<36:11,  2.37it/s, total=0.479, raw_losses=c:0.238/a:0.082/s:0.007, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.250(0.06)/a:0.499/s:1.000]2024-10-30 12:48:59 - INFO - \n",
            "Step 13000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:48:59 - INFO - Category pred mean: 0.075, target mean: 0.033\n",
            "Training:  74%|███████▍  | 13500/18146 [1:33:36<34:38,  2.24it/s, total=0.631, raw_losses=c:0.307/a:0.067/s:0.095, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.06)/a:0.432/s:0.250]2024-10-30 12:52:26 - INFO - \n",
            "Step 13500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:52:26 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  77%|███████▋  | 14000/18146 [1:37:04<29:31,  2.34it/s, total=0.620, raw_losses=c:0.277/a:0.081/s:0.135, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.06)/a:0.429/s:0.250]2024-10-30 12:55:54 - INFO - \n",
            "Step 14000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:55:54 - INFO - Category pred mean: 0.075, target mean: 0.033\n",
            "Training:  80%|███████▉  | 14500/18146 [1:40:32<25:48,  2.35it/s, total=0.602, raw_losses=c:0.329/a:0.052/s:0.011, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.05)/a:0.279/s:0.750]2024-10-30 12:59:23 - INFO - \n",
            "Step 14500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 12:59:23 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  83%|████████▎ | 15000/18146 [1:44:00<22:07,  2.37it/s, total=0.331, raw_losses=c:0.150/a:0.072/s:0.020, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.542(0.06)/a:0.414/s:0.750]2024-10-30 13:02:51 - INFO - \n",
            "Step 15000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 13:02:51 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  85%|████████▌ | 15500/18146 [1:47:27<18:20,  2.40it/s, total=0.557, raw_losses=c:0.224/a:0.064/s:0.205, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.125(0.05)/a:0.538/s:0.504]2024-10-30 13:06:18 - INFO - \n",
            "Step 15500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 13:06:18 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  88%|████████▊ | 16000/18146 [1:50:54<14:49,  2.41it/s, total=0.351, raw_losses=c:0.148/a:0.066/s:0.068, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.792(0.06)/a:0.411/s:0.507]2024-10-30 13:09:44 - INFO - \n",
            "Step 16000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 13:09:44 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  91%|█████████ | 16500/18146 [1:54:20<11:38,  2.36it/s, total=0.602, raw_losses=c:0.240/a:0.060/s:0.240, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.500(0.06)/a:0.466/s:0.250]2024-10-30 13:13:11 - INFO - \n",
            "Step 16500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 13:13:11 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  94%|█████████▎| 17000/18146 [1:57:48<08:01,  2.38it/s, total=0.482, raw_losses=c:0.142/a:0.084/s:0.270, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.875(0.05)/a:0.523/s:0.599]2024-10-30 13:16:39 - INFO - \n",
            "Step 17000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 13:16:39 - INFO - Category pred mean: 0.075, target mean: 0.033\n",
            "Training:  96%|█████████▋| 17500/18146 [2:01:15<04:32,  2.37it/s, total=0.324, raw_losses=c:0.158/a:0.047/s:0.028, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.667(0.04)/a:0.468/s:0.750]2024-10-30 13:20:06 - INFO - \n",
            "Step 17500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 13:20:06 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  99%|█████████▉| 18000/18146 [2:04:42<01:00,  2.40it/s, total=0.508, raw_losses=c:0.177/a:0.075/s:0.234, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.475(0.08)/a:0.450/s:0.069]2024-10-30 13:23:33 - INFO - \n",
            "Step 18000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 13:23:33 - INFO - Category pred mean: 0.075, target mean: 0.033\n",
            "Training: 100%|██████████| 18146/18146 [2:05:42<00:00,  2.41it/s, total=0.659, raw_losses=c:0.249/a:0.094/s:0.256, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.375(0.06)/a:0.528/s:0.500]\n",
            "Validating: 100%|██████████| 4537/4537 [18:19<00:00,  4.13it/s, total=0.138, cat=0.060(1.65), attr=0.039(1.00), seg=0.000(0.61), metrics=cat: 0.889, cat: 0.044, cat: 0.033, att: 0.774, seg: 1.000]\n",
            "2024-10-30 13:42:52 - INFO - \n",
            "Training Metrics:\n",
            "2024-10-30 13:42:52 - INFO - Total Loss: 0.5128\n",
            "2024-10-30 13:42:52 - INFO - Task Losses:\n",
            "2024-10-30 13:42:52 - INFO -   category: 0.2201 (weight: 1.6487)\n",
            "2024-10-30 13:42:52 - INFO -   attribute: 0.0697 (weight: 1.0000)\n",
            "2024-10-30 13:42:52 - INFO -   segmentation: 0.1323 (weight: 0.6065)\n",
            "2024-10-30 13:42:52 - INFO - Performance Metrics:\n",
            "2024-10-30 13:42:52 - INFO -   category_dice: 0.4306\n",
            "2024-10-30 13:42:52 - INFO -   cat_pred_mean: 0.0560\n",
            "2024-10-30 13:42:52 - INFO -   cat_target_mean: 0.0333\n",
            "2024-10-30 13:42:52 - INFO -   attribute_dice: 0.4782\n",
            "2024-10-30 13:42:52 - INFO -   segmentation_miou: 0.4943\n",
            "2024-10-30 13:42:52 - INFO - Train Loss: 0.5128\n",
            "2024-10-30 13:42:52 - INFO - Train Metrics: {'category_dice': '0.4306', 'cat_pred_mean': '0.0560', 'cat_target_mean': '0.0333', 'attribute_dice': '0.4782', 'segmentation_miou': '0.4943', 'category_loss': '0.2201', 'attribute_loss': '0.0697', 'segmentation_loss': '0.1323', 'category_weight': '1.6487', 'attribute_weight': '1.0000', 'segmentation_weight': '0.6065'}\n",
            "2024-10-30 13:42:52 - INFO - Val Loss: 0.4852\n",
            "2024-10-30 13:42:52 - INFO - Val Metrics: {'category_dice': '0.4728', 'cat_pred_mean': '0.0674', 'cat_target_mean': '0.0333', 'attribute_dice': '0.4859', 'segmentation_miou': '0.4875', 'category_loss': '0.2028', 'attribute_loss': '0.0692', 'segmentation_loss': '0.1343', 'category_weight': '1.6487', 'attribute_weight': '1.0000', 'segmentation_weight': '0.6065'}\n",
            "2024-10-30 13:42:52 - INFO - Learning Rate: 0.000009\n",
            "2024-10-30 13:42:53 - INFO - Saved checkpoint: DETR_CHECKPOINTS/fashion_multitask_full/checkpoint_latest.pth\n",
            "2024-10-30 13:42:54 - INFO - Saved best model: DETR_CHECKPOINTS/fashion_multitask_full/model_best.pth\n",
            "2024-10-30 13:42:54 - INFO - \n",
            "Epoch 5/5\n",
            "Training:   0%|          | 0/18146 [00:00<?, ?it/s]2024-10-30 13:42:56 - INFO - \n",
            "Step 0 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 13:42:56 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:   3%|▎         | 500/18146 [03:30<2:03:23,  2.38it/s, total=0.590, raw_losses=c:0.310/a:0.069/s:0.015, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.04)/a:0.595/s:0.750]2024-10-30 13:46:25 - INFO - \n",
            "Step 500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 13:46:25 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:   6%|▌         | 1000/18146 [06:57<2:00:31,  2.37it/s, total=0.422, raw_losses=c:0.216/a:0.049/s:0.029, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.03)/a:0.517/s:0.750]2024-10-30 13:49:52 - INFO - \n",
            "Step 1000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 13:49:52 - INFO - Category pred mean: 0.017, target mean: 0.033\n",
            "Training:   8%|▊         | 1500/18146 [10:25<1:56:43,  2.38it/s, total=0.505, raw_losses=c:0.176/a:0.075/s:0.231, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.06)/a:0.449/s:0.252]2024-10-30 13:53:20 - INFO - \n",
            "Step 1500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 13:53:20 - INFO - Category pred mean: 0.033, target mean: 0.033\n",
            "Training:  11%|█         | 2000/18146 [13:52<1:53:03,  2.38it/s, total=0.443, raw_losses=c:0.135/a:0.105/s:0.191, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.750(0.06)/a:0.494/s:0.250]2024-10-30 13:56:47 - INFO - \n",
            "Step 2000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 13:56:47 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  14%|█▍        | 2500/18146 [17:19<1:49:39,  2.38it/s, total=0.545, raw_losses=c:0.235/a:0.062/s:0.158, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.333(0.04)/a:0.623/s:0.250]2024-10-30 14:00:14 - INFO - \n",
            "Step 2500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:00:14 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  17%|█▋        | 3000/18146 [20:47<1:46:51,  2.36it/s, total=0.879, raw_losses=c:0.479/a:0.054/s:0.058, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.05)/a:0.349/s:0.250]2024-10-30 14:03:42 - INFO - \n",
            "Step 3000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:03:42 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  19%|█▉        | 3500/18146 [24:15<1:43:38,  2.36it/s, total=0.440, raw_losses=c:0.120/a:0.072/s:0.281, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.667(0.03)/a:0.490/s:0.500]2024-10-30 14:07:11 - INFO - \n",
            "Step 3500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:07:11 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  22%|██▏       | 4000/18146 [27:44<1:43:23,  2.28it/s, total=0.769, raw_losses=c:0.358/a:0.054/s:0.204, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.333(0.06)/a:0.453/s:0.252]2024-10-30 14:10:39 - INFO - \n",
            "Step 4000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:10:39 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  25%|██▍       | 4500/18146 [31:09<1:35:20,  2.39it/s, total=0.597, raw_losses=c:0.327/a:0.055/s:0.005, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.04)/a:0.524/s:1.000]2024-10-30 14:14:04 - INFO - \n",
            "Step 4500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:14:04 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  28%|██▊       | 5000/18146 [34:35<1:31:51,  2.39it/s, total=0.358, raw_losses=c:0.085/a:0.097/s:0.200, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.833(0.05)/a:0.613/s:0.500]2024-10-30 14:17:30 - INFO - \n",
            "Step 5000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:17:30 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  30%|███       | 5500/18146 [38:01<1:29:17,  2.36it/s, total=0.334, raw_losses=c:0.147/a:0.061/s:0.050, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.667(0.08)/a:0.233/s:0.500]2024-10-30 14:20:57 - INFO - \n",
            "Step 5500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:20:57 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  33%|███▎      | 6000/18146 [41:28<1:25:20,  2.37it/s, total=0.804, raw_losses=c:0.421/a:0.083/s:0.045, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.350(0.07)/a:0.564/s:0.500]2024-10-30 14:24:24 - INFO - \n",
            "Step 6000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:24:24 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  36%|███▌      | 6500/18146 [44:56<1:20:46,  2.40it/s, total=0.505, raw_losses=c:0.198/a:0.081/s:0.162, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.08)/a:0.436/s:0.608]2024-10-30 14:27:51 - INFO - \n",
            "Step 6500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:27:51 - INFO - Category pred mean: 0.075, target mean: 0.033\n",
            "Training:  39%|███▊      | 7000/18146 [48:23<1:17:57,  2.38it/s, total=0.348, raw_losses=c:0.169/a:0.070/s:0.000, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.708(0.07)/a:0.583/s:1.000]2024-10-30 14:31:18 - INFO - \n",
            "Step 7000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:31:18 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  41%|████▏     | 7500/18146 [51:51<1:15:24,  2.35it/s, total=0.725, raw_losses=c:0.376/a:0.072/s:0.054, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.05)/a:0.578/s:0.500]2024-10-30 14:34:46 - INFO - \n",
            "Step 7500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:34:46 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  44%|████▍     | 8000/18146 [55:19<1:11:02,  2.38it/s, total=0.648, raw_losses=c:0.254/a:0.084/s:0.240, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.05)/a:0.340/s:0.505]2024-10-30 14:38:14 - INFO - \n",
            "Step 8000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:38:14 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  47%|████▋     | 8500/18146 [58:45<1:07:24,  2.38it/s, total=0.385, raw_losses=c:0.143/a:0.068/s:0.133, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.750(0.07)/a:0.711/s:0.500]2024-10-30 14:41:41 - INFO - \n",
            "Step 8500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:41:41 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  50%|████▉     | 9000/18146 [1:02:13<1:04:13,  2.37it/s, total=0.717, raw_losses=c:0.370/a:0.086/s:0.035, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.100(0.08)/a:0.245/s:0.750]2024-10-30 14:45:08 - INFO - \n",
            "Step 9000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:45:08 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  52%|█████▏    | 9500/18146 [1:05:39<1:00:49,  2.37it/s, total=0.453, raw_losses=c:0.233/a:0.056/s:0.020, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.250(0.07)/a:0.250/s:0.750]2024-10-30 14:48:34 - INFO - \n",
            "Step 9500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:48:34 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  55%|█████▌    | 10000/18146 [1:09:06<59:01,  2.30it/s, total=0.936, raw_losses=c:0.431/a:0.098/s:0.209, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.292(0.08)/a:0.487/s:0.256] 2024-10-30 14:52:01 - INFO - \n",
            "Step 10000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:52:01 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  58%|█████▊    | 10500/18146 [1:12:33<53:06,  2.40it/s, total=0.346, raw_losses=c:0.169/a:0.067/s:0.000, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.08)/a:0.633/s:1.000]  2024-10-30 14:55:28 - INFO - \n",
            "Step 10500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:55:28 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  61%|██████    | 11000/18146 [1:16:00<49:49,  2.39it/s, total=0.735, raw_losses=c:0.307/a:0.069/s:0.263, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.500(0.06)/a:0.448/s:0.505]2024-10-30 14:58:55 - INFO - \n",
            "Step 11000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 14:58:55 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  63%|██████▎   | 11500/18146 [1:19:27<46:09,  2.40it/s, total=0.543, raw_losses=c:0.274/a:0.050/s:0.068, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.04)/a:0.359/s:0.750]2024-10-30 15:02:22 - INFO - \n",
            "Step 11500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 15:02:22 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  66%|██████▌   | 12000/18146 [1:22:54<42:58,  2.38it/s, total=0.392, raw_losses=c:0.152/a:0.070/s:0.116, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.667(0.08)/a:0.572/s:0.500]2024-10-30 15:05:49 - INFO - \n",
            "Step 12000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 15:05:49 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  69%|██████▉   | 12500/18146 [1:26:21<39:34,  2.38it/s, total=0.357, raw_losses=c:0.184/a:0.050/s:0.008, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.250(0.03)/a:0.463/s:1.000]2024-10-30 15:09:16 - INFO - \n",
            "Step 12500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 15:09:16 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  72%|███████▏  | 13000/18146 [1:29:49<36:48,  2.33it/s, total=0.714, raw_losses=c:0.390/a:0.056/s:0.026, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.000(0.06)/a:0.332/s:0.750]2024-10-30 15:12:44 - INFO - \n",
            "Step 13000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 15:12:44 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  74%|███████▍  | 13500/18146 [1:33:20<32:40,  2.37it/s, total=0.579, raw_losses=c:0.245/a:0.070/s:0.174, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.417(0.06)/a:0.439/s:0.253]2024-10-30 15:16:15 - INFO - \n",
            "Step 13500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 15:16:15 - INFO - Category pred mean: 0.075, target mean: 0.033\n",
            "Training:  77%|███████▋  | 14000/18146 [1:36:48<29:17,  2.36it/s, total=0.439, raw_losses=c:0.108/a:0.074/s:0.308, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.917(0.04)/a:0.542/s:0.476]2024-10-30 15:19:43 - INFO - \n",
            "Step 14000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 15:19:43 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  80%|███████▉  | 14500/18146 [1:40:16<25:42,  2.36it/s, total=0.619, raw_losses=c:0.262/a:0.095/s:0.153, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.333(0.06)/a:0.458/s:0.500]2024-10-30 15:23:11 - INFO - \n",
            "Step 14500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 15:23:11 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training:  83%|████████▎ | 15000/18146 [1:43:43<21:46,  2.41it/s, total=0.408, raw_losses=c:0.193/a:0.089/s:0.001, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.250(0.05)/a:0.360/s:1.000]2024-10-30 15:26:38 - INFO - \n",
            "Step 15000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 15:26:38 - INFO - Category pred mean: 0.042, target mean: 0.033\n",
            "Training:  85%|████████▌ | 15500/18146 [1:47:12<18:58,  2.32it/s, total=0.495, raw_losses=c:0.232/a:0.074/s:0.064, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.333(0.07)/a:0.569/s:0.500]2024-10-30 15:30:07 - INFO - \n",
            "Step 15500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 15:30:07 - INFO - Category pred mean: 0.058, target mean: 0.033\n",
            "Training:  88%|████████▊ | 16000/18146 [1:50:39<15:04,  2.37it/s, total=0.536, raw_losses=c:0.256/a:0.076/s:0.064, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.167(0.05)/a:0.622/s:0.500]2024-10-30 15:33:35 - INFO - \n",
            "Step 16000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 15:33:35 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  91%|█████████ | 16500/18146 [1:54:06<11:34,  2.37it/s, total=0.526, raw_losses=c:0.193/a:0.087/s:0.200, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.375(0.06)/a:0.312/s:0.000]2024-10-30 15:37:01 - INFO - \n",
            "Step 16500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 15:37:01 - INFO - Category pred mean: 0.050, target mean: 0.033\n",
            "Training:  94%|█████████▎| 17000/18146 [1:57:32<07:56,  2.41it/s, total=0.557, raw_losses=c:0.252/a:0.061/s:0.132, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.500(0.06)/a:0.426/s:0.750]2024-10-30 15:40:28 - INFO - \n",
            "Step 17000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 15:40:28 - INFO - Category pred mean: 0.092, target mean: 0.033\n",
            "Training:  96%|█████████▋| 17500/18146 [2:00:59<04:27,  2.41it/s, total=0.416, raw_losses=c:0.213/a:0.057/s:0.011, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.250(0.06)/a:0.427/s:0.750]2024-10-30 15:43:54 - INFO - \n",
            "Step 17500 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 15:43:54 - INFO - Category pred mean: 0.083, target mean: 0.033\n",
            "Training:  99%|█████████▉| 18000/18146 [2:04:25<01:02,  2.33it/s, total=0.555, raw_losses=c:0.255/a:0.101/s:0.053, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.542(0.07)/a:0.443/s:0.500]2024-10-30 15:47:21 - INFO - \n",
            "Step 18000 log_vars: [-0.5, 0.0, 0.5]\n",
            "2024-10-30 15:47:21 - INFO - Category pred mean: 0.067, target mean: 0.033\n",
            "Training: 100%|██████████| 18146/18146 [2:05:26<00:00,  2.41it/s, total=0.464, raw_losses=c:0.240/a:0.066/s:0.004, weights=c:1.649/a:1.000/s:0.607, metrics=c:0.333(0.04)/a:0.570/s:1.000]\n",
            "Validating: 100%|██████████| 4537/4537 [18:44<00:00,  4.03it/s, total=0.120, cat=0.048(1.65), attr=0.042(1.00), seg=0.000(0.61), metrics=cat: 0.889, cat: 0.044, cat: 0.033, att: 0.752, seg: 1.000]\n",
            "2024-10-30 16:07:06 - INFO - \n",
            "Training Metrics:\n",
            "2024-10-30 16:07:06 - INFO - Total Loss: 0.4936\n",
            "2024-10-30 16:07:06 - INFO - Task Losses:\n",
            "2024-10-30 16:07:06 - INFO -   category: 0.2109 (weight: 1.6487)\n",
            "2024-10-30 16:07:06 - INFO -   attribute: 0.0683 (weight: 1.0000)\n",
            "2024-10-30 16:07:06 - INFO -   segmentation: 0.1278 (weight: 0.6065)\n",
            "2024-10-30 16:07:06 - INFO - Performance Metrics:\n",
            "2024-10-30 16:07:06 - INFO -   category_dice: 0.4533\n",
            "2024-10-30 16:07:06 - INFO -   cat_pred_mean: 0.0581\n",
            "2024-10-30 16:07:06 - INFO -   cat_target_mean: 0.0333\n",
            "2024-10-30 16:07:06 - INFO -   attribute_dice: 0.4824\n",
            "2024-10-30 16:07:06 - INFO -   segmentation_miou: 0.4979\n",
            "2024-10-30 16:07:06 - INFO - Train Loss: 0.4936\n",
            "2024-10-30 16:07:06 - INFO - Train Metrics: {'category_dice': '0.4533', 'cat_pred_mean': '0.0581', 'cat_target_mean': '0.0333', 'attribute_dice': '0.4824', 'segmentation_miou': '0.4979', 'category_loss': '0.2109', 'attribute_loss': '0.0683', 'segmentation_loss': '0.1278', 'category_weight': '1.6487', 'attribute_weight': '1.0000', 'segmentation_weight': '0.6065'}\n",
            "2024-10-30 16:07:06 - INFO - Val Loss: 0.4748\n",
            "2024-10-30 16:07:06 - INFO - Val Metrics: {'category_dice': '0.4727', 'cat_pred_mean': '0.0669', 'cat_target_mean': '0.0333', 'attribute_dice': '0.4861', 'segmentation_miou': '0.4950', 'category_loss': '0.1989', 'attribute_loss': '0.0689', 'segmentation_loss': '0.1287', 'category_weight': '1.6487', 'attribute_weight': '1.0000', 'segmentation_weight': '0.6065'}\n",
            "2024-10-30 16:07:06 - INFO - Learning Rate: 0.000008\n",
            "2024-10-30 16:07:07 - INFO - Saved checkpoint: DETR_CHECKPOINTS/fashion_multitask_full/checkpoint_latest.pth\n",
            "2024-10-30 16:07:08 - INFO - Saved best model: DETR_CHECKPOINTS/fashion_multitask_full/model_best.pth\n",
            "2024-10-30 16:07:08 - INFO - Saved epoch checkpoint: DETR_CHECKPOINTS/fashion_multitask_full/checkpoint_epoch_5.pth\n",
            "2024-10-30 16:07:08 - INFO - \n",
            "Training completed successfully!\n",
            "2024-10-30 16:07:08 - INFO - ==================================================\n",
            "2024-10-30 16:07:08 - INFO - Total training time: 12.07 hours\n",
            "2024-10-30 16:07:08 - INFO - \n",
            "Final Training Metrics:\n",
            "2024-10-30 16:07:08 - INFO - category_dice: 0.4533\n",
            "2024-10-30 16:07:08 - INFO - cat_pred_mean: 0.0581\n",
            "2024-10-30 16:07:08 - INFO - cat_target_mean: 0.0333\n",
            "2024-10-30 16:07:08 - INFO - attribute_dice: 0.4824\n",
            "2024-10-30 16:07:08 - INFO - segmentation_miou: 0.4979\n",
            "2024-10-30 16:07:08 - INFO - category_loss: 0.2109\n",
            "2024-10-30 16:07:08 - INFO - attribute_loss: 0.0683\n",
            "2024-10-30 16:07:08 - INFO - segmentation_loss: 0.1278\n",
            "2024-10-30 16:07:08 - INFO - category_weight: 1.6487\n",
            "2024-10-30 16:07:08 - INFO - attribute_weight: 1.0000\n",
            "2024-10-30 16:07:08 - INFO - segmentation_weight: 0.6065\n",
            "2024-10-30 16:07:08 - INFO - \n",
            "Final Validation Metrics:\n",
            "2024-10-30 16:07:08 - INFO - category_dice: 0.4727\n",
            "2024-10-30 16:07:08 - INFO - cat_pred_mean: 0.0669\n",
            "2024-10-30 16:07:08 - INFO - cat_target_mean: 0.0333\n",
            "2024-10-30 16:07:08 - INFO - attribute_dice: 0.4861\n",
            "2024-10-30 16:07:08 - INFO - segmentation_miou: 0.4950\n",
            "2024-10-30 16:07:08 - INFO - category_loss: 0.1989\n",
            "2024-10-30 16:07:08 - INFO - attribute_loss: 0.0689\n",
            "2024-10-30 16:07:08 - INFO - segmentation_loss: 0.1287\n",
            "2024-10-30 16:07:08 - INFO - category_weight: 1.6487\n",
            "2024-10-30 16:07:08 - INFO - attribute_weight: 1.0000\n",
            "2024-10-30 16:07:08 - INFO - segmentation_weight: 0.6065\n",
            "2024-10-30 16:07:08 - ERROR - \n",
            "Error during training: 'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)\n",
            "2024-10-30 16:07:08 - ERROR - Traceback (most recent call last):\n",
            "  File \"/opt/conda/lib/python3.11/site-packages/matplotlib/style/core.py\", line 137, in use\n",
            "    style = _rc_params_in_file(style)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.11/site-packages/matplotlib/__init__.py\", line 870, in _rc_params_in_file\n",
            "    with _open_file_or_url(fname) as fd:\n",
            "  File \"/opt/conda/lib/python3.11/contextlib.py\", line 137, in __enter__\n",
            "    return next(self.gen)\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/opt/conda/lib/python3.11/site-packages/matplotlib/__init__.py\", line 847, in _open_file_or_url\n",
            "    with open(fname, encoding='utf-8') as f:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'seaborn'\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipykernel_109/3236839205.py\", line 212, in test_training_pipeline\n",
            "    fig = plot_training_history(history)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_109/3236839205.py\", line 18, in plot_training_history\n",
            "    plt.style.use('seaborn')\n",
            "  File \"/opt/conda/lib/python3.11/site-packages/matplotlib/style/core.py\", line 139, in use\n",
            "    raise OSError(\n",
            "OSError: 'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)\n",
            "\n",
            "2024-10-30 16:07:08 - INFO - Error state saved successfully\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training failed: 'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/matplotlib/style/core.py:137\u001b[0m, in \u001b[0;36muse\u001b[0;34m(style)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     style \u001b[38;5;241m=\u001b[39m _rc_params_in_file(style)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/matplotlib/__init__.py:870\u001b[0m, in \u001b[0;36m_rc_params_in_file\u001b[0;34m(fname, transform, fail_on_error)\u001b[0m\n\u001b[1;32m    869\u001b[0m rc_temp \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_or_url(fname) \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.11/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
            "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/matplotlib/__init__.py:847\u001b[0m, in \u001b[0;36m_open_file_or_url\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m    846\u001b[0m fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(fname)\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(fname, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m f\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'seaborn'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 251\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "Cell \u001b[0;32mIn[16], line 248\u001b[0m\n\u001b[1;32m    245\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m     history, trainer \u001b[38;5;241m=\u001b[39m test_training_pipeline()\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[16], line 239\u001b[0m, in \u001b[0;36mtest_training_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m save_error:\n\u001b[1;32m    237\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to save error state: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(save_error)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
            "Cell \u001b[0;32mIn[16], line 212\u001b[0m, in \u001b[0;36mtest_training_pipeline\u001b[0;34m()\u001b[0m\n\u001b[1;32m    209\u001b[0m metrics_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/metrics.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Plot and save training history\u001b[39;00m\n\u001b[0;32m--> 212\u001b[0m fig \u001b[38;5;241m=\u001b[39m plot_training_history(history)\n\u001b[1;32m    213\u001b[0m fig\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/training_history.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    214\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose(fig)\n",
            "Cell \u001b[0;32mIn[16], line 18\u001b[0m, in \u001b[0;36mplot_training_history\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_training_history\u001b[39m(history):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    Enhanced plotting function for training metrics\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     plt\u001b[38;5;241m.\u001b[39mstyle\u001b[38;5;241m.\u001b[39muse(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseaborn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m# Create subplots for each metric\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_metrics_history\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())\n",
            "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/matplotlib/style/core.py:139\u001b[0m, in \u001b[0;36muse\u001b[0;34m(style)\u001b[0m\n\u001b[1;32m    137\u001b[0m         style \u001b[38;5;241m=\u001b[39m _rc_params_in_file(style)\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 139\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    140\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstyle\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not a valid package style, path of style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile, URL of style file, or library style name (library \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstyles are listed in `style.available`)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    143\u001b[0m filtered \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m style:  \u001b[38;5;66;03m# don't trigger RcParams.__getitem__('backend')\u001b[39;00m\n",
            "\u001b[0;31mOSError\u001b[0m: 'seaborn' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in `style.available`)"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import traceback\n",
        "import pandas as pd\n",
        "from logging.handlers import RotatingFileHandler\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"\n",
        "    Enhanced plotting function for training metrics\n",
        "    \"\"\"\n",
        "    plt.style.use('seaborn')\n",
        "\n",
        "    # Create subplots for each metric\n",
        "    metrics = list(history['train_metrics_history'][0].keys())\n",
        "    num_plots = len(metrics) + 1  # +1 for loss plot\n",
        "    fig, axes = plt.subplots(num_plots, 1, figsize=(12, 4*num_plots))\n",
        "\n",
        "    # Add overall title with timestamp\n",
        "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "    fig.suptitle(f'Training Progress\\n{timestamp}', fontsize=16)\n",
        "\n",
        "    # Plot losses\n",
        "    axes[0].plot(history['train_losses'], 'b-o', label='Train Loss', markersize=4)\n",
        "    axes[0].plot(history['val_losses'], 'r-o', label='Val Loss', markersize=4)\n",
        "    axes[0].set_title('Training and Validation Losses')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].set_ylabel('Loss')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].legend(loc='upper right')\n",
        "\n",
        "    # Plot each metric\n",
        "    for i, metric in enumerate(metrics, 1):\n",
        "        train_metric = [metrics[metric] for metrics in history['train_metrics_history']]\n",
        "        val_metric = [metrics[metric] for metrics in history['val_metrics_history']]\n",
        "\n",
        "        axes[i].plot(train_metric, 'b-o', label=f'Train {metric}', markersize=4)\n",
        "        axes[i].plot(val_metric, 'r-o', label=f'Val {metric}', markersize=4)\n",
        "        axes[i].set_title(f'Training and Validation {metric}')\n",
        "        axes[i].set_xlabel('Epoch')\n",
        "        axes[i].set_ylabel(metric)\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "        axes[i].legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def setup_logging(experiment_name):\n",
        "    \"\"\"Setup logging configuration\"\"\"\n",
        "    # Create logs directory if it doesn't exist\n",
        "    os.makedirs('logs', exist_ok=True)\n",
        "\n",
        "    # Create timestamp for unique log file\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    log_filename = f'logs/{experiment_name}_{timestamp}.log'\n",
        "\n",
        "    # Add rotating file handler\n",
        "    file_handler = RotatingFileHandler(\n",
        "        log_filename,\n",
        "        maxBytes=10*1024*1024,  # 10MB\n",
        "        backupCount=5\n",
        "    )\n",
        "\n",
        "    console_handler = logging.StreamHandler()\n",
        "\n",
        "    # Enhanced formatting\n",
        "    formatter = logging.Formatter(\n",
        "        '%(asctime)s - %(levelname)s - %(message)s',\n",
        "        datefmt='%Y-%m-%d %H:%M:%S'\n",
        "    )\n",
        "\n",
        "    file_handler.setFormatter(formatter)\n",
        "    console_handler.setFormatter(formatter)\n",
        "\n",
        "    logger = logging.getLogger(__name__)\n",
        "    logger.setLevel(logging.INFO)\n",
        "    logger.addHandler(file_handler)\n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "    return logger\n",
        "\n",
        "def test_training_pipeline():\n",
        "    \"\"\"\n",
        "    Enhanced test pipeline with full dataset, checkpointing and logging\n",
        "    \"\"\"\n",
        "    # Setup experiment name and logging\n",
        "    experiment_name = 'fashion_multitask_full'\n",
        "    logger = setup_logging(experiment_name)\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    checkpoint_dir = f'DETR_CHECKPOINTS/{experiment_name}'\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    logger.info(\"Starting training pipeline\")\n",
        "    logger.info(\"=\" * 50)\n",
        "\n",
        "    # Log GPU information\n",
        "    if torch.cuda.is_available():\n",
        "        logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        logger.info(f\"Initial GPU Memory: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
        "\n",
        "    # Create train/val split\n",
        "    train_df, val_df = train_test_split(\n",
        "        df,\n",
        "        test_size=0.2,  # 20% for validation\n",
        "        random_state=42,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    train_df = train_df.reset_index(drop=True)\n",
        "    val_df = val_df.reset_index(drop=True)\n",
        "\n",
        "    # Validate dataset sizes\n",
        "    assert len(train_df) > 0, \"Training dataset is empty\"\n",
        "    assert len(val_df) > 0, \"Validation dataset is empty\"\n",
        "\n",
        "    # Print dataset information\n",
        "    logger.info(\"Dataset Information:\")\n",
        "    logger.info(\"-\" * 50)\n",
        "    logger.info(f\"Training data shape: {train_df.shape}\")\n",
        "    logger.info(f\"Validation data shape: {val_df.shape}\")\n",
        "    logger.info(\"-\" * 50)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = FashionMultiTaskDataset(\n",
        "        image_paths=[f\"{IMAGE_DIR}/{image_id}.jpg\" for image_id in train_df['ImageId']],\n",
        "        masks=train_df['EncodedPixels'],\n",
        "        categories=train_df['CategoryId'],\n",
        "        attributes=train_df['AttributesIds'],\n",
        "        image_processor=image_processor,\n",
        "        augment=True\n",
        "    )\n",
        "\n",
        "    val_dataset = FashionMultiTaskDataset(\n",
        "        image_paths=[f\"{IMAGE_DIR}/{image_id}.jpg\" for image_id in val_df['ImageId']],\n",
        "        masks=val_df['EncodedPixels'],\n",
        "        categories=val_df['CategoryId'],\n",
        "        attributes=val_df['AttributesIds'],\n",
        "        image_processor=image_processor,\n",
        "        augment=False\n",
        "    )\n",
        "\n",
        "\n",
        "    # Log model information\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    logger.info(\"\\nModel Information:\")\n",
        "    logger.info(f\"Total parameters: {total_params:,}\")\n",
        "    logger.info(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Initialize trainer with smaller batch size and gradient accumulation\n",
        "    trainer = MultitaskTrainer(\n",
        "        model=model,\n",
        "        train_dataset=train_dataset,\n",
        "        val_dataset=val_dataset,\n",
        "        batch_size=4,  # Reduced from 16\n",
        "        num_epochs=5,\n",
        "        learning_rate=2e-4,\n",
        "        patience=5,\n",
        "        max_grad_norm=1.0,\n",
        "        warmup_ratio=0.1,\n",
        "        checkpoint_dir=checkpoint_dir,\n",
        "        logger=logger,\n",
        "        gradient_accumulation_steps=4\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        logger.info(\"\\nStarting training...\")\n",
        "        logger.info(\"=\" * 50)\n",
        "\n",
        "        # Record start time\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Train the model\n",
        "        history = trainer.train()\n",
        "\n",
        "        # Calculate training time\n",
        "        training_time = time.time() - start_time\n",
        "\n",
        "        logger.info(\"\\nTraining completed successfully!\")\n",
        "        logger.info(\"=\" * 50)\n",
        "        logger.info(f\"Total training time: {training_time/3600:.2f} hours\")\n",
        "\n",
        "        # Log final metrics\n",
        "        logger.info(\"\\nFinal Training Metrics:\")\n",
        "        for metric, value in history['train_metrics_history'][-1].items():\n",
        "            logger.info(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "        logger.info(\"\\nFinal Validation Metrics:\")\n",
        "        for metric, value in history['val_metrics_history'][-1].items():\n",
        "            logger.info(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "        # Save metrics to CSV\n",
        "        metrics_df = pd.DataFrame({\n",
        "            'epoch': range(len(history['train_losses'])),\n",
        "            'train_loss': history['train_losses'],\n",
        "            'val_loss': history['val_losses'],\n",
        "            **{f'train_{k}': [h[k] for h in history['train_metrics_history']]\n",
        "               for k in history['train_metrics_history'][0].keys()},\n",
        "            **{f'val_{k}': [h[k] for h in history['val_metrics_history']]\n",
        "               for k in history['val_metrics_history'][0].keys()}\n",
        "        })\n",
        "        metrics_df.to_csv(f'{checkpoint_dir}/metrics.csv', index=False)\n",
        "\n",
        "        # Plot and save training history\n",
        "        fig = plot_training_history(history)\n",
        "        fig.savefig(f'{checkpoint_dir}/training_history.png')\n",
        "        plt.close(fig)\n",
        "\n",
        "        # Log final GPU memory usage\n",
        "        if torch.cuda.is_available():\n",
        "            logger.info(f\"Final GPU Memory: {torch.cuda.memory_allocated()/1024**2:.2f}MB\")\n",
        "\n",
        "        return history, trainer\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"\\nError during training: {str(e)}\")\n",
        "        logger.error(traceback.format_exc())\n",
        "\n",
        "        # Save error state\n",
        "        try:\n",
        "            torch.save({\n",
        "                'error_state': {\n",
        "                    'last_batch': batch if 'batch' in locals() else None,\n",
        "                    'last_outputs': outputs if 'outputs' in locals() else None,\n",
        "                    'model_state': model.state_dict()\n",
        "                }\n",
        "            }, f'{checkpoint_dir}/error_state.pth')\n",
        "            logger.info(\"Error state saved successfully\")\n",
        "        except Exception as save_error:\n",
        "            logger.error(f\"Failed to save error state: {str(save_error)}\")\n",
        "\n",
        "        raise e\n",
        "\n",
        "# Run the test pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    try:\n",
        "        history, trainer = test_training_pipeline()\n",
        "    except Exception as e:\n",
        "        print(f\"Training failed: {str(e)}\")\n",
        "        raise e"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "asSMEwZnSxy_",
        "2aokMJ4NSxzE",
        "qZnXJszoSxzE",
        "2Q3FgS3eSxzF",
        "FgIIn2u6SxzF",
        "Bi2Yr8ymSxzG",
        "6L4XE-_qSxzG",
        "xngwcFYaSxzH",
        "spgLYzTgSxzH"
      ],
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}